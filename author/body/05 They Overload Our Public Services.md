# Chapter 5: They Overload Our Public Services 

## The familiar accusation 

“They clog our hospitals. They flood our schools. They strain welfare.” These lines have echoed through time whenever a new wave of migrants arrives. In the twenty‑first century the rhetoric has been repurposed for a different kind of newcomer. The *Bloody Algos* are here, we are told, and they are overloading public services. The complaint is seductive because it feels familiar. Hospitals are already at breaking point, classrooms are crowded, welfare offices groan under the weight of forms. If an algorithm arrives promising to triage patients, grade essays or detect fraud, it seems obvious that we should welcome the help. When those systems fail, however, the charge shifts from “lazy humans” to “overworked machines,” and the blame once again falls on an outsider. 

Throughout this chapter we explore the myth that artificial immigrants are flooding our public infrastructure. We visit hospitals where algorithmic triage promised to democratize care yet delivered misdiagnoses and financial ruin. We walk into classrooms where AI proctors and grading tools were touted as neutral overseers but instead deepened inequities and eroded trust. We examine welfare systems where governments outsourced compassion to code, producing cruelty at scale. And we follow the money to see how technology vendors, consultants and politicians profit from the myth of efficiency while public services bear the costs. By the end we will see that the real overload is not an influx of algorithms but the offloading of responsibility onto systems that were never designed to carry it. 

## Healthcare on the algorithm’s clock 

When Babylon Health launched in the United Kingdom in 2017, it sold a familiar dream: on‑demand health care powered by artificial intelligence. Its chat interface would ask patients a series of questions, then deliver a diagnosis or triage suggestion without a physician present. The company claimed the system could handle minor issues, reducing strain on overworked general practitioners. National newspapers described the service as a “GP at hand,” and politicians championed it as an innovative way to modernize the National Health Service (NHS). Babylon’s founder, Ali Parsa, told the BBC that his AI could deliver safe advice at scale. The Conservative Party’s then health secretary Matt Hancock became an enthusiastic cheerleader, meeting with the company multiple times and accepting more than £250,000 in donations from individuals and firms associated with it. 

The reality was far less utopian. Doctors who tested Babylon’s chatbot found it performed like an elaborate decision tree, often missing serious symptoms. In one internal test an ingrown toenail was misdiagnosed as gout; in another, the system failed to recognize signs of a heart attack. The Lancet reviewed the tool and concluded there was “no evidence” it performed better than a human clinician, and some evidence it performed worse. Far from alleviating pressure, Babylon’s “GP at hand” service led to an *increase* in patient consultations. The NHS paid providers a flat fee per patient; because people used the service more frequently than expected, the company lost money on each interaction. It soon became clear that the algorithm promised speed but introduced blind spots. Triage decisions that would ordinarily be double‑checked by a nurse or doctor were automated, and patients who received reassurances from the app sometimes delayed seeking medical care. 

The financial fallout was no less dramatic. Babylon raised over £20 million from the NHS to develop its triage system and expanded quickly through private equity financing. By 2021 it had gone public in the United States, only to implode spectacularly. Leaked internal figures revealed a deficit approaching $900 million. In 2023 the company filed for bankruptcy protection. Its UK assets were sold to eMed Healthcare for a mere £500,000—a stunning collapse for a company once touted as the future of health care. For taxpayers the cost was not just the direct financial loss. Time and resources that could have strengthened primary care were diverted to a system that ultimately failed. 

Babylon is not the only example of algorithmic medicine overpromising and underdelivering. IBM’s “Watson for Oncology” was pitched as a system that would digest reams of medical literature and provide treatment recommendations faster than any human oncologist. Hospitals around the world bought licenses, and IBM touted the product in marketing campaigns. Yet internal documents revealed that Watson often suggested unsafe or incorrect treatments. A physician involved in testing the system described it as “a parlor trick” useful for marketing but not for patient care. Some of Watson’s errors stemmed from the limited data on which it was trained; others reflected that the system treated guidelines as static instructions rather than contextual recommendations. IBM quietly withdrew the product from many hospitals, but not before millions in public money had been spent. 

Perhaps the most painful lesson from these cases is not that AI will always fail but that healthcare cannot be “streamlined” by technology alone. Britain’s NHS has been chronically underfunded for more than a decade. Emergency departments have become so overcrowded that patients wait hours in corridors. Doctors and nurses are leaving the profession due to exhaustion and low pay. When an AI triage system misclassifies a heart attack or fails to spot sepsis, it isn’t an overloaded algorithm—it is a human‑made decision to substitute a computer for professional judgment. Babylon’s collapse shows how quickly hype can turn into harm when oversight is weak. For every patient misdiagnosed by a chatbot, there is a person who delayed seeing a doctor or lost faith in the system. The solution is not to ban all AI in healthcare but to integrate these tools within robust public services, with human clinicians responsible for every decision and adequate funding to match demand. 

## Automated classrooms 

Education is another arena where algorithms were promised as saviors. During the COVID‑19 pandemic, remote proctoring software became ubiquitous. Companies like Proctorio advertised tools that could monitor students through their webcams and flag “suspicious” behaviour, thereby preventing cheating and reducing the load on teachers. In practice the software often produced false positives and reinforced existing inequities. Civil rights advocates noted that AI proctoring tools harbor biases and present serious privacy risks. The American Civil Liberties Union argued that the technology was ineffective at catching cheaters yet inflicted “real harms” on students. At Utah’s online Kings Peak High School, a principal acknowledged that the cameras made students feel as though they were “watching [themselves] take the test in the mirror” and said some families were allowed to opt out due to privacy concerns. 

One particularly damning case came from a tech‑savvy college student who tested Proctorio’s face detection algorithm. Using a dataset of 11,000 images, he found the system failed to distinguish Black faces 57% of the time. Middle Eastern faces were misrecognized 41% of the time and white faces 40% of the time. A remote exam tool that cannot reliably identify students of different races not only undermines the fairness of assessments but also subjects Black and brown students to additional scrutiny. The company dismissed the findings as unrealistic, arguing that “children and cartoons” don’t take tests, yet it did not produce evidence that the algorithm was effective in real‑world conditions. Researchers studying AI proctoring concluded the software was “best compared to taking a placebo: it has some positive influence, not because it works but because people believe that it works”. 

The shortcomings of proctoring extend beyond race. In 2020 the UK government cancelled A‑level exams due to the pandemic and introduced an algorithm to assign grades. The system took teachers’ predicted grades and adjusted them based on a school’s historical performance. About 40 percent of predicted grades were downgraded. Students from disadvantaged areas were disproportionately affected; 10.4 percent of students from deprived backgrounds saw their grades fall compared with 8.3 percent of their peers. Private schools received a far higher proportion of top grades (48.6 percent) than comprehensive schools (21.8 percent). Public outcry forced the government to abandon the algorithm, but the fiasco left thousands of students anxious about university placements and job prospects. The immigrant analogy is striking: an outsider (the algorithm) was blamed for a decision that should never have been automated without proper safeguards. 

Beyond these headline failures, AI is steadily seeping into everyday classroom practices. By 2025, a report from the Gallup and Walton Family Foundation found that 60 percent of U.S. teachers used some form of AI tool during the 2024–25 school year. They reported saving nearly six hours per week on planning, grading and administrative tasks—time they could reinvest in personalized instruction and feedback. At Eastside Prep in Washington state, technology director Jonathan Briggs observed that “nobody really knows how to do this” yet. There is still not enough information about what kinds of AI use actually benefit students. When asked whether AI might help or harm human learning, he suggested that schools should focus on qualities that are hard to measure—collaboration, communication and creativity. In other words, the technology may boost performance in quantifiable subjects like math, but it cannot replace the struggle that makes learning meaningful. 

Teachers worry that generative AI could erode students’ ability to think for themselves. Chad Marsh, a veteran English teacher in the Lake Washington School District, observed that some students now feed bullet points into large language models and receive finished essays in return. Their writing “doesn’t evolve” because “someone has basically generated [their] thoughts”. At the same time, AI detection software that tries to catch such behaviour often flags students unfairly, creating an adversarial classroom atmosphere. Marsh believes AI should be integrated thoughtfully rather than ignored. This sentiment is echoed by state education authorities: the Washington Office of Superintendent of Public Instruction released a “Human‑AI‑Human” framework in early 2024, urging that AI “enhance, not replace, human learning”. Students themselves describe using AI for brainstorming, clarifying concepts and generating practice quizzes—but they also note that the tools can be inaccurate, rigid and may rob them of the “beauty of learning”. 

These diverse experiences reveal that AI does not simply overwhelm schools like a flood. It reconfigures how teachers teach, students learn and administrators make decisions. When used thoughtfully, AI can provide additional practice problems or free up time for teachers to engage more deeply with students. When deployed carelessly—whether in proctoring, grading or content creation—it can widen inequalities and erode trust. As with immigration, the scapegoating of AI hides deeper issues: under‑resourced schools, overcrowded classrooms, and a culture obsessed with quantifying achievement. Algorithms amplify these problems; they do not create them. 

## Welfare and bureaucracy 

In the world of social welfare, artificial immigrants have been asked to perform the role of gatekeeper. The Australian government’s Robodebt scheme is a cautionary tale. From 2016 to 2019 the Department of Human Services used income averaging algorithms to identify welfare recipients who allegedly owed money. The system compared annual income data from the tax office with fortnightly benefit payments and automatically generated debt notices when discrepancies were found. More than half a million inaccurate debts were raised. Recipients were told to prove they did not owe the money; if they could not provide payslips from years earlier, the debt was assumed valid. The scheme was declared unlawful by Australia’s Federal Court in 2019 because income averaging across an entire year to determine fortnightly entitlements was invalid. In practice the system reversed the presumption of innocence and placed the burden of proof on the vulnerable. 

The human cost was devastating. People on welfare received threatening debt letters and were instructed to repay thousands of dollars. Many took out loans to make the urgent repayments. The stress contributed to mental health issues and, tragically, several suicides. Eventually the government acknowledged that AU$746 million had been wrongfully recovered from 381,000 people and refunded the money. In 2020 a class action resulted in $1.75 billion of debts being wiped. The scheme was described by the Robodebt royal commission as a “crude and cruel mechanism, neither fair nor legal”. Yet even after these findings, accountability remained elusive. In September 2025 the federal government agreed to pay an additional $475 million in compensation, bringing total redress to more than $2.4 billion. One class action member remarked that the settlement recognised the devastating impacts but could never fully heal the wounds. A partner at the law firm handling the case noted the irony: “One of the problems brought about by Robodebt was people being treated like numbers”—a reminder that dehumanization was baked into the algorithm. 

The Netherlands offers another cautionary example. The Dutch tax authority introduced an algorithmic system to detect childcare benefit fraud. The model relied on risk factors such as nationality and ethnicity. Tens of thousands of parents—many of whom were low income or immigrants—were falsely accused of fraud. Their benefits were suspended, leading to financial distress, mounting debt and mental health problems. Amnesty International described the system as xenophobic; families were targeted because they held dual nationality. The scandal became so severe that the government resigned in 2021. Here the metaphor of AI as an immigrant is inverted: the algorithm acted like a border agent, deciding who was “eligible” for support based on nationality. And just like the border, the system was opaque and unaccountable. 

These welfare scandals reveal a pattern: governments often adopt algorithms not to improve services but to reduce costs. Automated systems allow officials to claim they are “modernizing” bureaucracy while quietly shifting the burden onto citizens. They invert the presumption of innocence and strip away the nuance that caseworkers bring to complex situations. The scapegoat narrative—that AI itself is overloading welfare offices—deflects blame from policymakers who starve social systems of resources and oversight. 

## How AI companies leach money 

While the public endures misdiagnoses, unfair grades, and unjust debts, private companies have discovered a lucrative market in selling technological quick fixes to governments. Promises of efficiency and savings mask a deeper reality: these systems rarely deliver what was advertised, yet the contracts keep coming. The same names appear again and again—consultancies, platform vendors, data firms. Many patterns emerge across sectors.

## Consultancy capture and pilot projects that never scale 

Large consulting firms often act as intermediaries between public agencies and AI vendors. They promise transformation through pilot projects and feasibility studies. In the UK the NHS commissioned a *Federated Data Platform* (FDP) to integrate patient data across hospitals. The contract was awarded to Palantir, a U.S. surveillance and data analysis firm. Despite the hype, freedom‑of‑information requests show that by early 2025 only 34 NHS trusts—less than 15 percent—were actively using the platform. Over half of responding trusts had not adopted it at all. Many described Palantir’s technology as a step backwards from existing systems. The contract’s details were shrouded in secrecy; three‑quarters of its 586 pages were redacted, prompting legal action. Originally adoption was optional, but in mid‑2024 senior NHS executives wrote to trust leaders requiring them to plan adoption within two years. By February 2025, NHS England claimed that 96 trusts had “signed up,” yet an FOI response revealed that only 34 were actually using the software while the rest had merely “signalled their intent”. Consultants were paid £8.5 million to promote the system while many hospitals cut management staff. 

IBM’s Watson for Oncology and Babylon Health also illustrate the “pilot project” trap. Hospitals and health agencies spend millions on trials that rarely translate into routine use. When the projects fail, vendors can blame “implementation challenges,” pocket the funds and move on. Public organisations, lacking in‑house technical capacity, become dependent on external consultants. This dynamic echoes stories of colonization: resources are extracted while promises of development remain unfulfilled. 

## Vendor lock‑in and perpetual fees 

Another way AI companies profit is by locking public agencies into proprietary systems. Palantir’s Foundry software has been criticised for making integration with common data‑science tools difficult. Critics warn that once a hospital trust invests in Palantir’s infrastructure, switching providers becomes costly. In effect, the company inserts itself into the heart of public health systems, making future budgets contingent on continued licensing and upgrades. As one analyst noted, the NHS could build its own data platform but lacks the funding and capacity; spending half a billion pounds on an unproven system risks long‑term dependency. 

## Efficiency that cuts service and creates downstream costs 

AI tools promise to “streamline” services by reducing human labor. In practice, the costs of errors and appeals fall on the public. When the UK’s A‑level grading algorithm produced unfair results, appeals clogged up hotlines and forced universities to overhaul admissions. When Robodebt miscalculated debts, tens of thousands of people filed appeals and class actions, leading to billions in compensation. These downstream costs far outweigh any short‑term savings. Similarly, misdiagnoses by Babylon and Watson for Oncology required human doctors to intervene, sometimes after patients had suffered harm. In each case, the public health system paid twice: once for the faulty technology and again to clean up its mess. 

## Surveillance and data monetization 

Some AI vendors derive revenue from data harvested through public contracts. Proctorio and its competitors collect video, audio and biometric data from students. Palantir’s contracts with the NHS, the Ministry of Defence and police forces give it access to sensitive personal information. Investigations revealed that Palantir was working on a “real‑time data‑sharing network” with police forces that would process information including people’s health, sexual orientation, sexual activity, trade union membership, race, religion and political beliefs. Critics fear this creates a surveillance infrastructure that can be repurposed beyond healthcare or education. In other words, companies profit twice: first by selling software and second by exploiting data. 

## Revolving door politics and lack of liability 

Political support for AI projects is often lubricated by donations and lobbying. Babylon’s meteoric rise was accompanied by over £250,000 in donations to the Conservative Party and high‑profile endorsements from ministers. Consultancies like KPMG and companies like Palantir employ former government officials who know how to navigate procurement rules. When projects fail, liability often falls on the public purse. IBM and Babylon did not compensate patients who received wrong advice; the cost of misdiagnoses was borne by individuals and the NHS. In the case of Robodebt, officials who designed the scheme were not held accountable. The lack of liability encourages risky experimentation with vulnerable populations. 

## Appeals and oversight costs 

Finally, AI systems create new bureaucratic layers. When an algorithm denies someone a benefit or miscalculates a grade, there must be a process to challenge the decision. Appeals committees, ombudsmen and courts get involved. The cost of adjudicating errors can exceed the savings achieved by automation. In the Netherlands thousands of families spent years fighting for justice. In Australia class actions and royal commissions swallowed millions. Each appeal is a reminder that algorithmic efficiency is often a mirage; by the time errors are corrected, the damage is already done. 

## The real overload 

If AI is not clogging hospitals or flooding schools, what is? Chronic underinvestment, political neglect and social inequality. In the NHS, waiting lists and staff shortages predate the introduction of AI. The algorithmic triage offered by Babylon did not overwhelm the system; it exposed the lack of capacity to deliver timely care. In education, overfilled classrooms, high teacher turnover and testing regimes created pressure long before chatbots and proctoring software arrived. Automated grading and surveillance did not cause the crisis; they simply shifted the burden. And in welfare, the Robodebt fiasco did not arise because an AI was overworked; it happened because policymakers chose to implement an algorithm that inverted the presumption of innocence and ignored warnings from experts. The real overload lies in the erosion of trust and the waste of public money. When people cannot understand or challenge decisions that affect their lives, they withdraw from civic life. When billions are spent on systems that do not work, cynicism grows. 

The immigrant metaphor helps reveal the scapegoat. Throughout history elites have blamed immigrants for strains on public services when the underlying issue was deliberate austerity. Similarly, blaming algorithms for “overloading” healthcare, education or welfare distracts us from the policy choices that starve those systems. We are told that technology will make services more efficient; when it fails, the narrative shifts to cautionary tales about rogue machines. In reality humans set the rules, allocate budgets and decide whether to invest in capacity or outsource to private vendors. The overload is not a natural disaster caused by AI. It is a man‑made consequence of austerity and deregulation. 

## What integration should look like 

How might we welcome artificial immigrants without subjecting ourselves to their whims? The answer lies in integration, not assimilation or abandonment. AI systems should be colleagues, not overlords. That means embedding them in processes where human professionals retain ultimate responsibility. A triage chatbot can suggest possible causes for a cough, but a nurse or doctor must review the recommendation. An essay‑scoring program can identify grammar errors, but teachers should read the work and use the tool as a guide. A fraud detection algorithm can flag anomalies, but caseworkers must investigate before sending debt notices. 

Integration also requires transparency and explainability. Public institutions should procure systems whose logic can be audited. Palantir’s contract secrecy and proprietary algorithms undermine democratic oversight. Governments should require vendors to disclose training data, weighting factors and error rates. When algorithms are used in high‑stakes decisions, individuals should have the right to an explanation and the ability to appeal to a human. The Washington state education guidance emphasises a “human‑AI‑human” loop: students should initiate and interpret AI use. This model should apply across public services. 

Another pillar of integration is investment in public digital capacity. Rather than outsourcing to multinational firms, governments could develop in‑house technical teams. The NHS’s reliance on Palantir stems from decades of underfunding its IT infrastructure. Building public platforms would not only reduce vendor lock‑in but also allow citizens to shape technology according to democratic values. The same holds for education: universities and schools could collaborate on open‑source grading tools that reflect pedagogical values rather than surveillance. Welfare agencies could design simple verification systems that respect dignity and privacy. 

Finally, integration demands democratic governance. Communities should be involved in deciding when and how AI is used. Patients, teachers, students, welfare recipients and civil rights advocates must have seats at the table. Impact assessments should examine not only technical accuracy but also social consequences. In the Netherlands the disaster with the childcare benefits algorithm spurred calls for bans on racial profiling and human rights impact assessments. Similar mechanisms could be adopted elsewhere. Such engagement treats AI not as an alien invader but as a tool that can be shaped and controlled. 

## How we care for each other should never be abandoned to automation 

Hospitals, schools, and welfare offices are where democracy touches flesh. They are the places where society shows whether every person matters—or doesn’t. When we hand decisions in these spaces to opaque algorithms, we risk hollowing out the human foundations of care, teaching, and solidarity. The real danger isn’t that AI overloads public services; it’s that we allow technology to strip out the compassion that makes them humane. The immigrant metaphor helps: new arrivals can enrich a community when there are resources and goodwill; they become scapegoats when systems are already strained. Likewise, AI can assist if integrated thoughtfully within well-funded institutions. But if we treat algorithms as cheap replacements for nurses, teachers, or caseworkers, we will condemn ourselves to a future of automated neglect and permanent mistrust. Some domains—personal care above all—demand the touch of another human being. The choice is ours. 

