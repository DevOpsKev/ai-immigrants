# Chapter 6: They Bring Crime and Disorder

## Phantom callers and shapeshifters

An 82‑year‑old woman in the Midlands picks up her landline and hears her grandson sobbing.  He has been in an accident and needs money fast.  The voice on the other end knows his nickname, the make of his first car, and the pub where he watched the World Cup last summer.  It is convincing because it is him – harvested from hours of social‑media video and re‑stitched by a generative model that can mimic a person down to the cadence of their breathing.  The woman wires thousands of pounds to a stranger and only later learns that her grandson was never in danger.  This is not science fiction.  Police across Europe have warned that voice‑cloning scams are surging, costing families millions and eroding trust in the most intimate spaces of life [REF: voice‑cloning scam report].

We often talk about immigrants “bringing crime” as if criminality is a property that travels with bodies across borders.  The fear echoes in contemporary debates about artificial intelligence.  Machine‑generated scripts can draft phishing emails free of grammatical errors and at a scale no human spammer could manage.  Models that once generated Shakespearean sonnets now churn out ransomware scripts and romance‑scam narratives.  The sense of violation is heightened by the technology’s ability to impersonate loved ones or authority figures, to appear everywhere and nowhere at once.  In these phantom calls and shapeshifting messages, AI becomes the perfect con artist – not because it wants to, but because somebody has trained it to serve that purpose.

Yet even here, the immigrant analogy clarifies.  When a gang exploits the vulnerabilities of a grandmother, the criminal is not the synthetic voice but the human gang leader, the money launderer, the platform that sells access to voice‑cloning tools.  As with human migrants, blaming the tool distracts from the organised networks that profit from deception.  If society panics over “AI scams” without examining the incentives and guardrails of telecom providers, social‑media companies, and payment processors, then we repeat the historical mistake of blaming the newest arrival rather than the system that enables wrongdoing.

## Deepfakes and stolen selves

Identity theft used to mean stolen passports and pilfered bank details.  In 2025 it often means that your face, voice, or writing style is fed into a machine and turned into an asset you do not control.  Deepfake technology, which can synthesize realistic video of anyone doing anything, has moved from hobbyist novelty to professional weapon.  A British journalist discovered a sexually explicit video circulating online featuring her likeness.  She never posed for it.  The perpetrators used publicly available images and open‑source software to create a convincing forgery [REF: deepfake harassment case].  Women politicians and activists have faced similar attacks, often designed to intimidate them out of public life.  This form of gendered abuse echoes the use of forged photographs and rumours to discredit immigrants in previous centuries.  Now, the fabrication is instant, global and nearly impossible to remove.

Copyright and privacy law have been slow to adapt.  Creators have filed lawsuits against companies that train models on their songs, artworks or books without consent.  Comedians and novelists discovered that their voices and writing styles were being imitated by chatbots that answered questions as if they were the original authors.  In 2024 a group of authors brought a class‑action suit against OpenAI and Meta for scraping their books to train large language models without licence [REF: copyright class action].  Photographers and visual artists joined similar actions when they found AI‑generated images replicating distinctive styles.  The legal question is as much about moral rights as about money: does a machine that draws from millions of human expressions owe its creators anything?  The companies argue that training on public data is “fair use”; the artists retort that building a multi‑billion‑pound industry on uncompensated labour smacks of colonial extraction.

Beyond the creative realm, deepfake forgeries threaten democracy itself.  Shortly before an election, a video appears showing a candidate making racist remarks or confessing to corruption.  Even if debunked within hours, the damage may be done – the clip spreads faster than the correction.  Disinformation campaigns deploy these tools to sow distrust in public institutions.  The European Union’s elections agency warned that deepfakes could be used to suppress turnout or provoke violence [REF: election integrity report].  The spectre of digital forgeries evokes long‑standing fears about immigrants introducing disorder, but here the immigrants are streams of data crossing borders unmoored from accountability.  The line between protected speech and malicious manipulation blurs when anyone can create plausible evidence of anything.

## Hacking with a side of machine learning

Cybercrime has always been an arms race.  In the 1990s it was script kiddies and viruses in email attachments.  By the 2010s it was sophisticated ransomware gangs holding hospitals hostage.  Today the game includes machine‑generated code that can discover vulnerabilities, craft bespoke malware, or test thousands of phishing variations against corporate spam filters until one gets through.  Researchers demonstrated in 2024 that large language models could be instructed to refine malicious code beyond the abilities of novice programmers [REF: AI‑assisted hacking study].  Criminals have seized on these capabilities.  Dark‑web forums advertise “AI worms” that automatically adapt to evade detection, and chatbots that sell stolen credit‑card numbers complete with scripts to bypass verification.

This escalation exposes a myth.  We sometimes imagine that machines are the actors, “breaking in” and “stealing data.”  But machines neither plan nor profit.  The decision to deploy an AI‑powered intrusion comes from a person or a group.  They invest in models because detection tools have improved, and the returns justify the risk.  Meanwhile, the companies building defensive AI – antivirus firms, cloud providers, cybersecurity contractors – market their own machine‑learning solutions as silver bullets.  The result is an arms economy in which both offence and defence are fuelled by AI hype and venture capital.  The victims are hospitals unable to access patient data, small businesses locked out of their accounts, and local governments forced to pay ransoms to restore essential services.

It bears remembering that most successful hacks still start with a human error: a weak password, an unpatched server, an employee tricked by a cleverly worded email.  AI can amplify these attacks by tailoring them, but it does not change the underlying vulnerability.  Calling AI the culprit masks how organisations underinvest in security and how criminals exploit poverty and desperation to recruit coders.  If we treat the technology as an uncontrollable criminal, we may neglect basic cyber hygiene and regulation.  As in earlier chapters, the immigrant metaphor teaches caution: demonising the outsider deflects scrutiny from the insiders who set the rules.

## Algorithmic coercion and harmful influence

Not all AI‑related harm arrives through scams and hacks.  Some emerges from the ways algorithms shape what we see, think and do.  Social‑media recommendation systems decide which videos appear in a teenager’s feed.  A 14‑year‑old struggling with body image may be served an endless stream of weight‑loss tips and extreme dieting videos.  A young man searching for answers about his future may be pulled into a rabbit hole of misogynistic “manosphere” content and conspiracy theories.  In 2023 the US Surgeon General warned that social media poses a “profound risk of harm” to youth mental health [REF: surgeon general report].  Studies found that algorithmic ranking systems amplify content that provokes strong emotions – anger, fear, desire – because that is what keeps users engaged.  Companies that run these platforms know it.  Former employees of TikTok, Facebook and YouTube have testified that internal experiments showed how tweaks to recommendation algorithms could reduce self‑harm content or extremist propaganda but would also lower engagement metrics and, by extension, advertising revenue [REF: platform whistleblower testimony].

AI can also be used explicitly for coercion.  Governments and extremist groups have deployed chatbots that befriend vulnerable individuals in online forums and gradually steer them toward violence or self‑harm.  A UN report on online radicalisation notes that algorithmic profiles can identify users with grievances and then expose them to materials that justify violence against perceived enemies [REF: UN radicalisation report].  In private chat apps, abusers use machine‑learning tools to stalk partners, predict their movements, and manipulate them with tailored messages.  These abuses are not accidents; they are design choices or, at minimum, tolerated side effects of business models that prioritise engagement over well‑being.

The immigrant metaphor resurfaces when pundits describe “AI addicts” as if technology itself enslaves people.  Addiction is real, but its drivers are economic and psychological.  Social‑media companies test thousands of algorithmic configurations to maximise time spent on their platforms.  They hire behavioural scientists to engineer “variable reward” loops reminiscent of casino slot machines.  Blaming the algorithm for encouraging self‑harm or extremism is like blaming a migrant worker for the poverty that forces him into dangerous labour.  The focus on individual responsibility or machine will obscures the structural incentives that make coercive design profitable.

## Black boxes and hidden hands

Serious harm arises when AI is deployed in systems of power without transparency or accountability.  Consider the sophisticated surveillance tools used by authoritarian regimes and ostensibly democratic governments alike.  In western China, facial‑recognition cameras coupled with predictive‑policing algorithms have been used to track Uyghur Muslims, flag “pre‑criminal” behaviour and feed mass detention campaigns [REF: surveillance and human rights report].  Human‑rights groups have labelled this a crime against humanity.  The technology itself does not discriminate; it is the human policy that defines whose face triggers an alert and whose does not.  When companies supply these systems, they often shroud the details in trade secrecy.  Victims cannot challenge the logic of an algorithm that they are not allowed to inspect.

In Israel’s 2023 military campaign in Gaza, investigative journalists revealed that an AI‑powered targeting platform nicknamed “Lavender” identified thousands of suspected militants and generated lists for airstrikes with minimal human oversight [REF: AI warfare investigation].  Former Israeli intelligence officials admitted that the system’s tolerance for collateral damage was set politically: civilian homes were counted as acceptable targets if they were within a certain radius of a suspected combatant.  The result was mass civilian death and widespread destruction.  The tragedy underscores how AI can supercharge violence when decision‑makers lower ethical thresholds.  The algorithm did not commit war crimes; commanders and politicians did.  But its opacity and scale made it easier to kill without reflection.

Even in peacetime, black‑box systems undermine rights.  Predictive‑policing tools deployed in American and British cities have directed police patrols to neighbourhoods predominantly inhabited by minorities, based on historical arrest data [REF: predictive policing study].  That data reflects decades of over‑policing and discrimination.  Vendors claim proprietary rights over their algorithms and resist disclosing how they weigh variables.  When activists sought to challenge risk scores in court, they hit a wall of corporate secrecy.  In 2024 the US state of Arkansas considered a bill that would criminalise revealing the source code of election‑management systems – a move that would further insulate algorithmic processes from public scrutiny [REF: election technology secrecy bill].  The pattern is clear: when companies control both the data and the software, they wield outsized influence over public life and shield themselves from accountability by invoking intellectual property.

Opacity is not confined to repressive governments or private contractors.  Even the models powering everyday chatbots like ChatGPT remain largely inscrutable.  OpenAI’s decision not to release training datasets for GPT‑4 or its successors has sparked criticism from researchers and civil‑rights groups, who argue that such secrecy makes it impossible to audit biases or understand potential harms.  When governments sign contracts with vendors to deploy these systems in schools, welfare offices or courtrooms, they effectively outsource public authority to black boxes.  History shows that immigrants are often met with suspicion and demands to “prove they belong”; AI systems, by contrast, are granted extraordinary authority without equivalent scrutiny.

## Copycats and cultural theft

While criminal scams and surveillance are headline‑grabbing, there is a slower, quieter form of harm: the expropriation of creative labour.  In the nineteenth century, British textile mills copied patterns from Indian artisans and mass‑produced them for European markets, crushing local crafts.  Today’s models scrape the internet for images and text, digesting centuries of human culture and spitting out synthetic imitations.  A Kenyan illustrator finds her unique style replicated by an AI that learned from thousands of her Instagram posts.  A New Zealand musician hears a synthetic track in his voice on a streaming platform where he has never uploaded a song.  The harm is not just financial; it is a sense of dispossession.  When art becomes training data, the artist loses control over how her expression circulates.

This theft extends beyond individuals.  Indigenous communities have seen sacred symbols and stories reproduced by AI art generators that know nothing of their cultural significance.  Museums in the Global North digitise artefacts taken during colonial times and make them available to AI companies without consulting source communities.  The models treat cultural heritage as raw material in a global database, much like colonisers once treated land and bodies as resources to be exploited.  The companies building these systems – often headquartered in California or Shenzhen – frame the practice as “democratising creativity.”  They seldom acknowledge the asymmetries of power at play or offer restitution.

Intellectual‑property law struggles to keep up.  In 2025 the British government opened a consultation on whether text and data mining for AI training should fall under a copyright exception [REF: UK copyright consultation].  Tech firms argued that any restriction would hamper innovation; artists warned that without safeguards they would be forced to abandon their crafts.  The parallel with immigration debates is uncanny: when textile factories undercut handloom weavers in nineteenth‑century Bengal, industrialists justified it as progress.  The weavers were told to adapt or starve.  Today, creative workers face a similar ultimatum: accept that machines will learn from you without pay, or risk being left behind.  The real question is who sets the terms of that exchange and who benefits from the wealth generated.

## Follow the money

Across scams, hacks, surveillance and cultural appropriation, one pattern recurs: there is always a human beneficiary.  The teenager tricking grandparents with cloned voices may be acting out of desperation, but the engineers who built the cloning software do so because investors see profit.  The ransomware gang hires AI coders because healthcare providers will pay to restore patient records.  Defence contractors sell automated targeting systems because militaries fund them generously.  Social‑media companies prioritise engagement even when it harms mental health because advertising revenue depends on attention.  Large language‑model companies treat open‑internet content as free training data because the legal risk seems lower than paying for licences.

In human immigration debates, crime is often exaggerated to justify exclusion.  Politicians highlight violent incidents involving migrants while ignoring data showing that immigrants are less likely to commit crimes than native‑born citizens [REF: crime statistics comparison].  The same tactic appears in AI discourse.  Headlines about rogue chatbots and killer robots feed fear that the technology itself is out of control.  Yet most harm arises from predictable incentives: criminals seek easier ways to steal, corporations seek cheaper sources of content and labour, and governments seek tools to maintain power.  Focusing on “AI crime” without naming these motives allows offenders to hide behind the mystique of the machine.

Naming names matters.  When Clearview AI scraped billions of images from social media to build a facial‑recognition database, police departments in the US and UK quietly signed contracts even though the company had been fined by regulators for privacy violations.  When Amazon sold Rekognition to law‑enforcement agencies, activists discovered the system misidentified women and people of colour at higher rates [REF: facial recognition bias study].  When Palantir pitched its predictive‑policing platform to local governments, the deals were often shielded by non‑disclosure agreements and no‑bid contracts.  These are not stories of machines running amok; they are choices made in boardrooms and city councils, motivated by profit, power and, sometimes, fear.  Accountability begins with identifying who decides to deploy a tool, under what oversight, and for whose benefit.

## Toward safe harbours

The litany of harms described in this chapter could leave one feeling hopeless.  If AI enables scams, hacks, surveillance, coercion and cultural theft, perhaps we should ban it altogether.  But history shows that banning “outsiders” rarely solves underlying problems.  The way forward lies in reclaiming agency and building structures that protect rights while harnessing benefits.

On the personal level, digital literacy and scepticism are crucial.  Just as communities teach new immigrants how to avoid predatory landlords or exploitative employers, we must teach citizens how to recognise deepfakes, challenge suspicious requests, and demand transparency.  Financial institutions can implement verification steps that cannot be spoofed by cloned voices, such as code words or biometric checks.  Schools and parents can advocate for platform design changes that prioritise safety over engagement, and regulators can compel companies to audit and mitigate harmful recommendation patterns.

At the institutional level, the principles of human rights offer a framework.  The Universal Declaration of Human Rights prohibits arbitrary detention and guarantees privacy.  These protections must extend to digital contexts.  Governments should not deploy predictive‑policing tools without rigorous assessment of their impact on marginalized communities.  International law scholars argue that AI‑enabled war crimes demand new accountability mechanisms – perhaps akin to the Geneva Conventions but tailored to algorithmic weapons systems [REF: AI and international law].  Creative communities, meanwhile, are experimenting with licensing frameworks that allow artists to decide whether and how their work contributes to training datasets, sharing in the value generated.

Transparency is a precondition for all of this.  That means open records on how algorithms make decisions, public registers of AI systems used by governments, and whistle‑blower protections for employees who expose harmful practices.  It also means resisting trade‑secret claims that override democratic oversight when public safety or rights are at stake.  The push for explainable AI cannot solve every problem – many models are too complex for full interpretability – but it can ensure that the humans deploying them remain answerable.

Finally, we should remember that newcomers enrich societies when they are welcomed with support and clear rules.  Immigrants bring new cuisines, art and ideas when given the opportunity to thrive.  AI, too, can augment human creativity, detect fraud, accelerate scientific discovery and help us adapt to climate change.  The challenge is not to fear it or worship it, but to integrate it deliberately.  Integration means designing for human dignity, compensating those whose data and labour fuel innovation, regulating abuses, and centring those most vulnerable to harm.  When we do that, we turn a source of disorder into an opportunity for collective growth.