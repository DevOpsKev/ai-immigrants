# Ruining Our Culture 

*The voice of the immigrant is often heard before their person is seen.* Think of the first time an alien rhythm crossed the Atlantic: jazz sailing from New Orleans to Paris, reggae filtering through London’s council flats, manga zines passed hand to hand on American campuses. Each new arrival was met with warnings of corruption and dilution—concerns that the home culture would be *ruined*. The fear is older than any technology. In the nineteenth century, intellectuals fretted that cheap novels would rot morals; by the twentieth, rock and hip‑hop were supposedly contaminating youth. Today’s immigrant is not human at all. It is generative artificial intelligence—*algos*, invisible newcomers that remix our art, speech and sense of reality. They sail the undersea cables with offerings of music, images and stories. And many of us feel a familiar anxiety: are the machines *ruining* our culture, or are they simply the latest foreigners to stretch its boundaries? 

## Jetpacks and blindfolds: AI as cultural immigrant 

In early 2024 MIT’s Laboratory for Social Machines convened artists and architects to discuss how generative AI might reshape design. Researcher Ziv Epstein compared using AI to “putting on a jetpack and a blindfold”: it sends you hurtling forward, yet you don’t know where you’re going. Architect Ana Miljački used an image generator to produce “synthetic memories” of Yugoslav monuments. She found that the resulting forms suggested nostalgia for a past that never existed, raising questions about *who* remembers when AI synthesizes memory. Panelists agreed that generative models are not neutral. They derive patterns from vast datasets dominated by Western male creators and can replicate those biases. This cultural weighting means the “immigrant” arrives carrying the baggage of an uneven world; it imitates the language and aesthetics of those who have had the loudest voices. 

The metaphor of AI as an immigrant invites empathy and critique. Like newcomers to a nation, generative models learn by watching and imitating—consuming billions of words, songs and images and then trying to fit in. They are built on statistics, not personal experience. Yet they produce outputs that evoke emotions we reserve for human creation: joy at a new song, awe at a painted landscape, laughter at a joke. They also trigger anxieties that echo the xenophobic tropes of the past. When early jazz reached Europe, critics feared its “primitive” rhythms would corrupt civilised music. Today we worry that AI art will erode authenticity; that machine‑generated media will drown out human voices; that synthetic influencers and chatbots will replace us. Whether these fears are justified or another chapter in a long saga of cultural evolution is one of the central questions of our era. 

## Remixing music: copyright battles and new voices 

Music often functions as a bellwether for cultural change. The generative music startup Suno allowed people to type in prompts like “a ballad about love in the style of 1980s rock” and receive a polished song within seconds. Between December 2023 and mid‑2024, the company said its generator was used 12 million times. Udio, a rival, offered similarly slick results. Both companies trained their systems on copyrighted recordings without permission but argued that such learning was “fair use,” likening it to a child absorbing music by listening to records. The Recording Industry Association of America disagreed. In June 2024 it sued Suno and Udio, accusing them of “unlicensed copying of sound recordings” and seeking damages up to $150,000 per work. The RIAA warned that AI companies who scrape artists’ life’s work without consent threaten the promise of innovation. In reply, Udio insisted that helping people generate new artistic expression is what copyright law is meant to encourage. 

These lawsuits are the latest round in a broader battle. In 2023 a viral track called “Heart on My Sleeve” used AI to synthesize vocals resembling Drake and the Weeknd. It racked up millions of streams before Universal Music Group demanded its removal. The label said the episode showed why platforms have a responsibility to prevent “infringing content created with generative AI”. The same year, the RIAA warned that AI companies were violating copyrights “en masse” and that fans might one day “no longer enjoy music by their favourite artists because those artists can no longer earn a living”[. On the other side, some labels are experimenting with collaboration: reports in 2025 suggest companies like Universal are considering licensing catalogues to Suno and Udio to get a paid cut when their music is used. Spotify, after quietly removing tens of thousands of AI‑generated tracks in 2023, later said it would welcome AI‑made music as long as it was produced legally. Like immigration policy, the path forward may involve visas and work permits—licenses and attribution systems that acknowledge both the original artists and the new algorithmic performers. 

## Visual arts and the “style list” scandal 

Generative image models have been celebrated for their ability to conjure breathtaking scenes from a sentence. Yet behind the marvel lies a history of appropriation. In January 2024 a leaked spreadsheet revealed that MidJourney, one of the most popular image generators, had trained on the styles of more than 16,000 artists without permission. The list included Frida Kahlo, Jean‑Michel Basquiat and Walt Disney. Artists were outraged. They argued that the company commodified their signature styles without compensation, enabling users to generate endless imitations. Tools like Glaze emerged to help artists mask their works so that AI models could not easily copy them. 

Artists took the dispute to court. In August 2024 a U.S. judge allowed a class‑action lawsuit against Stability AI, Midjourney and DeviantArt to proceed. The court ruled that storing and using artists’ works to train AI could plausibly constitute copyright infringemen. The plaintiffs argued that Stable Diffusion facilitates infringement by design. This decision, though preliminary, signals that the legal system is wrestling with how to balance innovation and intellectual property rights. As with previous waves of cultural “imports”—think of how hip‑hop producers sampled soul records before copyright law adjusted—the question is not whether appropriation will happen but how society will recognize and compensate the creators whose work fuels new art forms. 

## Sora and the simulated film set 

When OpenAI unveiled its text‑to‑video model *Sora* in February 2024, the film industry experienced a flash of both wonder and dread. The model could generate minute‑long, high‑fidelity videos that mimic complex camera movements and realistic physical phenomena. It could even extend existing footage. In a world where Hollywood budgets are inflated by location shoots, set construction and special effects, a tool that conjures a cityscape from a prompt is a direct threat. Film producers quickly realized that scenes requiring expensive green screens or travel could be simulated with a few keystrokes. Tyler Perry reportedly paused a $800 million studio expansion after seeing Sora’s capabilities. 

Sora’s launch was carefully controlled. OpenAI restricted prompts that might produce extreme violence, sexual content or hateful imagery, and it claimed to use only publicly available and licensed training data. Even so, critics worry that the training set includes copyrighted material and that the model could enable misinformation by creating realistic fake news footage. Brookings scholars note that Sora “heightens risks to the creative industry” because it can replace human roles—actors, set designers, even screenwriters—and because it raises unresolved copyright questions. The case exemplifies the immigrant analogy: Sora is a newcomer with extraordinary talents. It could enrich the cinematic commons, offering independent filmmakers new tools to tell their stories. It could also displace workers and prompt the exploitation of existing art without consent, much as earlier waves of mechanization displaced artisans. Whether we embrace or regulate this immigrant will determine the future of film. 

## Ghost writers and authenticity panic 

Generative AI does more than create finished works; it also destabilizes notions of authorship. One of the early observations from the identity professionals’ field is that AI flips the roles of author and editor: the machine produces a draft, and the human merely edits. This reversal raises ethical questions. Should a student claim authorship of a paper written by ChatGPT? Are employees who use AI to draft emails still the authors? The U.S. Copyright Office has grappled with these questions, repeatedly denying copyright protection to works entirely generated by AI and calling for transparency in mixed collaborations. The IDPro article notes that generative models can produce plausible citations and undercut academic integrity. 

A study by Adobe found that *authenticity panic* is widespread. Ninety‑four percent of consumers worry that misinformation will affect elections; 74 percent have doubted the authenticity of photos or videos even from reputable news sources; and 93 percent want to know how a piece of digital content was made. In response, the Content Authenticity Initiative promotes “Content Credentials”—metadata that functions like a nutrition label, indicating whether AI was used in creation. Major camera makers and even the U.S. Department of Defense have adopted this system. Yet provenance tools are only as good as their adoption. Deepfakes have become so convincing that even detection systems struggle. A Columbia Journalism Review piece chronicled the case of a high school principal who was falsely accused after an AI‑generated voice recording circulated; experts found that existing audio deepfake detectors work well only within specific domains and are easily evaded. Meanwhile, the Stanford AI Index 2025 recorded a 56 percent increase in AI‑related incidents, including deepfake intimate images and chatbots implicated in a teenager’s suicide. The immigrant metaphor again proves apt: we are anxious not just about new cultural forms but about the strangers who may pass as natives. 

## Humor, memes and the algorithmic mirror 

Humor is often considered a distinctly human trait, yet AI is learning to mock and meme alongside us. On TikTok and WhatsApp, algorithms watch our laughter. A study from the University of Sydney found that there is an 85 percent chance you will use the laughing‑crying emoji when you find something funny. These signals become metadata that teaches algorithms what people find amusing. Chatbots like “Witscript” attempt to generate jokes; human evaluators labelled the AI’s output as jokes about 40 percent of the time. Humor‑enabled avatars such as ERICA and Jess can exhibit comedic timing, but replicating authentic laughter remains a complex challenge—the researchers noted that capturing the subtleties of human laughter will take years. Laughter is a cultural glue; when we outsource it to machines, we risk replacing a communal ritual with a simulation. 

Internet memes illustrate how quickly cultural references evolve. Generative models can produce endless variations on a meme template. Yet part of what makes a meme funny is the shared understanding of its context. When AI generates jokes, it often misses nuance or lacks the lived experience behind the humor. As with any immigrant learning a new language, it may mispronounce or misapply idioms. Our willingness to laugh at AI jokes might signal that we value relatability more than origin. Or it might reveal how quickly we adapt, letting machines into our in‑jokes. 

## Code and craftsmanship 

For software engineers, generative AI appears to offer a productivity boost. Tools like GitHub Copilot and ChatGPT can spit out code on demand. Yet the reality is complicated. A 2024 Stack Overflow essay argues that “writing code is the easiest part of software engineering.” Generative AI can produce volumes of code, but it does not help to understand, test or integrate that code into complex systems. The essay compares AI to a junior engineer who types very fast. The code may seem correct, but you still have to verify it line by line. In some cases, using AI may slow down development because the time saved on typing is lost in debugging. 

Empirical data supports this caution. Machine Evaluation for Realists (METR) conducted a randomized controlled trial in mid‑2025 on experienced open‑source developers. Participants using AI assistants were on average *19 percent slower* than those coding unaided. The kicker: the AI‑assisted developers believed they were *24 percent faster*, illustrating a gap between perception and reality. This is reminiscent of earlier technological “immigrants” like the power loom or assembly line. They promised efficiency but reshaped labour in unintended ways. AI may democratize coding by making it accessible to more people, but it also risks flooding repositories with unmaintainable code and eroding the craft of programming. 

## Linguistic justice and endangered tongues 

Language is another domain where AI’s cultural footprint is huge. More than 7,000 languages exist, yet generative models are primarily trained on a handful of data‑rich languages like English, Spanish and Mandarin. There are over 150 dialects of English alone, but most AI systems default to mainstream American grammar. Linguist Wittgenstein wrote that “the limits of my language mean the limits of my world.” The Brookings Institution warns that the digital language divide will widen if AI tools marginalize speakers of under-resourced languages. Without intentional inclusion, AI will homogenize language, correcting “colour” to “color” and smoothing out the cadences of Nigerian Pidgin and Singlish. The Neville Hobson blog summarizing a Conversation article reports that roughly 90 percent of generative AI training data is in English and mostly mainstream American English. The author notes that these systems often misrecognize non‑standard dialects and may correct Indian English words like *prepone* (to move a meeting earlier) to a more “acceptable” form. We risk losing linguistic diversity by algorithmic assimilation, the digital equivalent of forbidding immigrants to speak their native tongue. 

Yet AI also offers tools for language preservation. In 2025 researchers at Dartmouth worked with speakers of Nüshu, a centuries‑old Chinese script used by women, to build a translation model using just 35 sentence pairs. They used GPT‑4 to generate additional resources and cross‑trained the model with Mandarin. The project, called **NüshuRescue**, allowed participants to read, write and sing in the script. The study warns, however, that such tools can unintentionally introduce bias from dominant cultures; community participation is necessary to ensure authenticity. Meanwhile, Google Translate famously misidentified Navajo sentences as Irish or Turkish, underscoring the need for better support of under‑represented languages. Generative AI could be a lifeline for endangered languages, but only if we treat their speakers as collaborators rather than data points. 

## Synthetic influencers and hyperreal celebrities 

In the realm of social media, AI is not just composing posts but embodying personas. Brands like Balmain introduced virtual models that never age or eat and found they could cut marketing costs by 40 percent. These synthetic influencers are available 24/7, avoid scandals (unless programmed otherwise), and deliver hyper‑personalized marketing. Gucci launched its own AI muse, while chatbots like Kuki amassed millions of followers. TechNow reports that industries from fashion and gaming to finance are adopting virtual influencers and that future trends include hybrid “human clones,” voice and personality customization, and AI‑generated celebrities. The article notes ethical concerns: deepfake misuse, data privacy risks, job displacement and an authenticity crisis. 

Disrupt Marketing warns that deepfake technology is already undermining trust in influencer marketing. A fake ad showing podcaster Joe Rogan promoting a libido booster circulated widely; streamers have suffered identity theft through AI‑generated pornographic clips; and platforms like Twitch have been forced to update policies. The Influencer Marketing Hub surveyed industry experts and found that 28.4 percent considered deepfake fraud and AI‑generated content a major challenge, while 24.6 percent were concerned about authenticity erosion. This is reminiscent of the early days of Photoshop when retouched magazine covers sparked debates about unrealistic beauty standards. The difference is scale: AI can automate the production of endless synthetic personas, saturating feeds with perfect faces and curated opinions. It is as if hundreds of new celebrities arrived overnight with no backstory and no hometown. 

## Deepfake detection and the arms race 

The proliferation of synthetic media leads to a cat‑and‑mouse game between creators and detectors. Journalism’s role—to verify and contextualize—has never been more threatened. Columbia Journalism Review recounts the case of a school principal falsely accused after an AI‑generated voice message claimed he propositioned a student. When investigators used publicly available deepfake detectors, they found that the tools were effective only on examples similar to the training set. New voices or recording conditions easily bypassed detection. The piece quotes experts who warn that audio detection methods are “utterly brittle”; visual deepfake detectors face similar problems. With generative models improving rapidly, detection may always lag behind. 

This arms race affects trust beyond journalism. Elections, public health messaging and corporate communications can all be subverted by convincing fake audio and video. In 2024 NewsGuard counted over 1,200 websites created entirely by AI that churned out misinformation with no human oversight. The Stanford AI Index recorded 233 AI-related incidents in 2024, a 56 percent increase from the previous year, including deepfake intimate images and chatbots implicated in suicide. Faced with this flood, our notion of truth begins to drift. If we cannot trust our eyes and ears, culture becomes a hall of mirrors. The immigrant analogy suggests another parallel: just as foreign communities were once scapegoated for social ills, we may blame “the algorithms” for problems that stem from our own media ecosystems and regulatory failures. 

## Whose archives, whose memory? 

Generative AI’s ability to synthesize images and narratives from training data makes it a powerful tool for constructing collective memory. Yet this process is inherently selective. Which archives are digitized and used as training data? Who decides what becomes part of AI’s cultural memory? The MIT panelists described generative AI as an “ontological wrecking ball,” capable of both preserving and erasing heritage. Miljački’s synthetic Yugoslav monuments raised the specter of machine‑generated nostalgia, where people might cherish AI fabrications of a past they never lived through. Similarly, when GPT‑4o launched image generation in March 2025, users flooded social media with anime‑style portraits and Studio Ghibli‑inspired landscapes. The White House even posted a Ghibli‑style image of a migrant’s arrest, prompting criticism that the administration was trivializing a human tragedy. Hayao Miyazaki, the co‑founder of Studio Ghibli, has long been critical of AI art; seeing his aesthetic used in official political communication felt like a step beyond appropriation into propaganda. 

GPT‑4o’s rollout also sparked the *Scarlett Johansson controversy*. One of its voices—“Sky”—sounded so similar to Johansson that the actress’s husband joked about it on Saturday Night Live. OpenAI paused the voice after public backlash, and Johansson revealed that the company had tried to license her voice months earlier but she declined. She expressed shock that OpenAI still released a voice that “sounded so eerily similar to mine,” highlighting the lack of legal safeguards around using someone’s likeness. This episode echoes debates about the right of publicity: do we own the sound of our voices? When generative models treat voices and styles as raw material, they blur the boundary between homage and theft. In migrant terms, the newcomer might wear our clothes and speak in our accent without asking, raising questions about consent and respect. 

## The meaning of creative work 

So is AI *ruining* our culture? The answer depends on how we define culture. If culture is a static canon to be preserved untouched, then any algorithmic remix might seem like vandalism. This view fuels nostalgia for a “pure” past that never truly existed; every era has borrowed, stolen and hybridized. Blues came from the collision of African rhythms and European instruments. K‑pop and reggaetón are global mosaics. Culture, in this sense, thrives on immigration. 

If culture is instead a living process of shared meaning-making, then generative AI is both a tool and a participant. It democratizes creativity by lowering the barrier to making songs, stories and images. A teenager can spin up a psychedelic album or design an avatar without formal training. Minority communities can use AI to document and revitalize endangered languages. At the same time, the speed and scale of AI production threaten to flood the commons with synthetic content, drown out slower human voices, and entrench the biases of dominant cultures. The challenge, then, is governance and values. We need mechanisms—licensing regimes, provenance tags, labor protections—to ensure that artists are compensated and audiences can tell who made what. 

The immigrant metaphor reminds us that assimilation is not one-way. Newcomers change the host culture, and the host culture changes the newcomers. Jazz musicians Europeanized ragtime rhythms; salsa fused Cuban son with New York jazz; manga influenced Western comics and vice versa. Today’s algos draw on our archives to generate something new, but we also design and tune them. We decide what data they ingest, how we deploy them, and whether we treat them as collaborators or tools. We can push for linguistic justice by training models on diverse dialects; we can demand fair compensation for artists; we can teach children to discern deepfakes. 

## What makes culture matter? 

At the end of this exploration, a provocative question emerges: *If our culture can be simulated, what made it matter in the first place?* When AI can compose symphonies, paint masterpieces and crack jokes, we are forced to clarify why human creativity is valuable. Perhaps it’s not just the final artifact but the intention, context and relationships behind it. We cherish a song not only because of its melody but because we know someone sweated over every note. We treasure a dialect because it carries the history of a community. If generative AI floods the world with passable replicas, the scarcity of human stories may become our most precious resource. 

The immigrant analogy offers hope. Historically, cultural exchanges that once seemed threatening ultimately enriched societies. America’s love affair with sushi did not erase hamburgers; British punk did not replace classical music. Instead, these forms co‑existed and inspired new hybrids. Our challenge with AI is to build institutions and norms that allow the algos to contribute without erasing the people who teach them. That means recognizing the labour and rights of human creators, supporting minority languages and art forms, and developing ethical frameworks that balance openness with accountability. It also means embracing the flux of culture itself. The *bloody algos* are here—messy, disruptive, creative. Whether they ruin our culture or help it evolve is ultimately up to us. 