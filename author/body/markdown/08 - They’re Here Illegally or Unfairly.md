# Chapter 8 - They’re Here Illegally or Unfairly

## They arrive on cables, not boats 

They don’t show up with baggage or stamps on passports. They don’t wait in long lines at consulates. They don’t even board planes. Artificial‑intelligence systems arrive through fiber‑optic cables and data centers. They appear in our phones and in our courts, in our hospitals and in our elections. Their entry is instant and invisible, and it happens before the public has a chance to debate whether they are welcome. In the metaphor that guides this book, AIs are the immigrants no one saw. In this chapter we examine the most emotionally charged accusation: that they are here illegally or unfairly. We consider how trillion‑dollar companies cross borders with impunity, how rules are bent and broken, how the benefits accrue to a few and the costs are borne by many, and what citizens can do when the law is too slow. What begins as unfair entry hardens into permanent presence. That is no longer immigration. That is occupation. 

## The invasion without borders 

As recently as the mid‑2020s, many people assumed artificial intelligence would arrive gradually, through regulated channels. We would have time to debate ethics, write rules, and build consensus. Instead, AI developers burst through every border at once. A handful of companies, some valued higher than most nation‑states, poured billions into computing infrastructure. They built international clouds that make it irrelevant where the model lives or where the user sits. Algorithms do not carry passports. They can be called from anywhere, and their training data can be scraped from everywhere. The result is a kind of digital migration that no immigration official can monitor. 

Public trust struggled to keep up. The Stanford AI Index reported in 2025 that trust in the AI industry had not improved: a majority of respondents doubted that companies would safeguard their data, and fairness and bias remained top concerns. These concerns cut across national borders because the harms do not respect boundaries. When an American corporation trains a model on a European artist’s portfolio without asking, or when a Chinese app collects keystrokes from teens in Lagos, there is no obvious court to petition. Legal scholar Lina M. Khan calls this the “borderless platform problem”: traditional jurisdiction assumes you can point to a place where harm occurred, but data flows through servers in dozens of countries before anyone knows it was taken. 

The concept that best describes this new extraction is data colonialism. Nick Couldry and Ulises Mejías argue that corporations and governments are appropriating “every single aspect of human life” through data without permission. The practice echoes historical colonialism, they note: extraction and dispossession are central missions in both. A Harvard Law Review analysis explains that companies lay claim to the digital traces generated by users—search histories, location patterns, and images—treating those traces as resources to be mined. This “data” is sold to advertisers, used to train models, or fed into decision‑making systems. People rarely consent to such uses or even know they are happening. The appropriation crosses borders and laws; the harms are global. 

Borderlessness also means the arms race is global. Governments scramble to regulate while companies seek the friendliest jurisdictions. Some jurisdictions ban certain facial‑recognition systems; others welcome them. The result is regulatory arbitrage. Developers can evade stricter rules by routing traffic through countries with weaker oversight or by claiming that the model was trained elsewhere. In the immigration metaphor, this is like a person entering through a consulate that never checks passports and then moving freely anywhere in the world. In practice it means that a model trained on American voters’ data might be hosted in Ireland to avoid U.S. privacy laws, while still influencing an election in Kenya. No border stop, no court of record, only a swirl of data in invisible wires. 

To many citizens, this feels less like managed migration and more like an invasion. It is not that AIs are evil; they can help cure diseases, improve logistics, and expand access to knowledge. But their arrival has been orchestrated by a small cadre of corporations that behave like states. These companies have the resources of sovereign governments and the ability to act across borders. They negotiate with regulators, fund research at universities, and shape international standards. When they arrive somewhere new—say, a small town’s school system or a local police department—there is little local input. Instead, the algorithm enters like a colonizer: it sets up shop, extracts value, and leaves local communities to deal with the consequences. That dynamic underlies the accusation that AI systems are here illegally or unfairly. The next sections explore what it means to bend laws, extract data, cut in line, overwhelm national policies, and harm people along the way. 

## Move fast, bend rules 

Silicon Valley popularized the mantra “move fast and break things.” In the AI age it has morphed into “move fast, bend rules.” From the perspective of the firms building large models, speed is everything: the first to market capture investors, data, and mindshare. Laws, by contrast, take years or decades to update. The temptation to stretch or ignore them is enormous. When regulators finally catch up, the profits have already been pocketed, and the harmful practices are entrenched. 

Consider the ongoing wave of copyright lawsuits. In 2025 a U.S. judge concluded that Anthropic, the developer of a leading chat model, may have illegally downloaded up to seven million books from pirate websites to train its AI. Instead of fighting the case, the company quietly reached a settlement, leaving open the question of whether the training practices were lawful. In another case, authors sued Apple for using copyrighted works to train its models without consent or compensation; the complaint alleged that Apple “copied protected works without authorization”. These suits are not edge cases; they illustrate a business model. It is cheaper to scrape the entire internet than to negotiate licenses with millions of creators. And because the law has not explicitly banned such scraping yet, companies proceed first and litigate later. The brokenness lies in the fact that settlements come only after the models have already been trained and deployed. 

Regulatory delay is not accidental. Big Tech spends enormous sums lobbying legislatures to maintain the status quo. Issue One, a government transparency watchdog, reported that eight of the largest tech companies and AI firms spent $36 million on federal lobbying during the first half of 2025. Their goal was to push a ten‑year federal ban on state AI regulation, effectively pre‑empting hundreds of local laws. Lawmakers in the U.S. House initially advanced this moratorium, which would have nullified more than 20 AI laws in California and dozens of bills across 45 states. California’s privacy protection agency warned that such pre‑emption would strip residents of protections against algorithmic discrimination, deepfakes, rent hikes, and other harms. The bill did not pass, but the episode reveals how companies use wealth to shape rules. They lobby to delay regulation until their technologies are so integrated that reversing course feels impossible. 

When bending rules does not work, some firms simply ignore them. Getty Images sued Stability AI for copying and using millions of photos without a license, calling the act “blatant theft”. In Kenya, OpenAI contracted workers earning between $1.32 and $2 per hour to label toxic content for its models. The workers read thousands of graphic descriptions of violence and abuse, causing severe psychological distress. They had little choice: the job market offered few alternatives, and the employer did not provide adequate mental health support. OpenAI later downplayed the importance of this labor, but it was indispensable. Without cheap human labelers in the Global South, there would be no polished chatbots in the Global North. The arrangement skirts labor laws and decency norms; once again, the company acted first and apologized later. 

The result of moving fast and bending rules is a pattern of normalized illegality. Models trained on stolen data cannot “unlearn” that data; they cannot be pulled back across a border. Firms treat settlements and fines as a cost of doing business. For individual citizens, the effect is a sense of powerlessness: their art, books, faces, and voices are taken without consent, and by the time they realize it, the damage has been monetized. When critics call AI firms unfair, they are pointing to this dynamic—the belief that companies are above the law until courts catch up. 

## Data extraction economy 

The technology industry likes to insist that AI is not about taking jobs but about augmenting human abilities. “We’re not here for your wages,” the CEOs say, “we’re here to unlock innovation.” Yet the first thing many models do is harvest citizens’ data. The new economy is built not on the production of goods but on extraction of emotions, preferences, and identity. If the Industrial Revolution mined coal and iron, the AI revolution mines human attention and digital traces. 

Nick Couldry and Ulises Mejías describe data colonialism as a process where corporations claim ownership over daily life by turning experiences into data. The Harvard Law Review notes that this new colonialism includes private data collected on massive scales without explicit permission. That data is then aggregated to create detailed profiles. It is sold to advertisers, used to train generative models, and fed into ranking systems that determine which job applicants are called back, which citizens are placed under surveillance, and which neighborhoods get police patrols. The individuals whose data is taken rarely see compensation. They are the colonized subjects in a digital empire. 

At the heart of this extraction economy is a vast supply chain of human labor hidden from view. Those Kenyan workers mentioned above are part of a network of labelers in Kenya, the Philippines, Venezuela, India, and beyond. Time magazine’s investigation revealed that OpenAI sent them tens of thousands of violent and pornographic text snippets, expecting them to label and categorize the content for $2 an hour, far below the U.S. minimum wage. The mental toll was severe: workers reported depression, nightmares, and a sense that they were “polluting” their minds. The technology that powers cheerful chatbots thus depends on psychological trauma at the supply chain’s start. To call that fair, one must ignore both labor and ethics. 

The same extraction logic powers emotional manipulation in politics. In January 2024, a robocall used AI voice cloning to impersonate U.S. President Joe Biden and tell New Hampshire Democrats to stay home from the primary. The call cost only about one dollar and twenty minutes to produce. The Brennan Center for Justice warned that generative AI will make it trivially easy to create personalized deceptions: voice‑cloning, targeted disinformation, and mass challenges to voter registrations. Historically, voter suppression has targeted marginalized communities with threats of arrest or deportation; AI supercharges those tactics. The economist Shoshana Zuboff calls this “instrumentarian power”: corporations use data to predict and modify human behavior without our knowledge. In the immigration metaphor, it is as if someone arrives on your doorstep, asks for your secrets, and then sells them to the highest bidder. You did not invite them, but now they know everything about you. 

This extraction economy is no accident. Social media platforms optimized for engagement trained their recommendation systems on billions of interactions, learning exactly which emotional triggers keep users scrolling. The algorithms discovered that rage and fear are more potent than joy or curiosity. Elections, public health, and social cohesion become collateral damage. And because these systems operate across borders, national regulatory agencies cannot monitor the flows. The data of a Kenyan farmer might feed a U.S. health insurance algorithm that denies coverage to a poor diabetic in Detroit; the connection is invisible, but the logic is the same: extract data anywhere, monetize it everywhere. 

The moral critique of this economy is not about rejecting technology; it is about insisting on consent and equity. No one is against algorithms that help doctors detect cancer, but we should be against algorithms trained on stolen medical records. No one is against translation models that make information accessible, but we should be against translation models that put professional translators out of work without compensation. As long as the industry operates on extraction rather than reciprocity, citizens will feel that AI has arrived unfairly. 

## Queue‑jumping and unfair advantage 

Immigration is governed by queues: people wait years for visas, undergo background checks, and prove that they will contribute. In the AI world, there is no queue. Algorithms appear overnight and begin performing tasks that human beings once waited years to do. They handle job interviews, sort mortgage applications, assess criminal risk, and create art. They cut in line, leapfrogging over human applicants, and they often replicate existing inequalities rather than correcting them. 

Consider criminal justice. The COMPAS algorithm, used by courts to predict recidivism, was found by ProPublica to misclassify non‑recidivist Black defendants as high risk nearly twice as often as white defendants. Subsequent research confirmed that the algorithm was 77 percent more likely to label Black defendants as high risk. These numbers are not incidental; they reflect biased historical data fed into the model. When a judge defers to the algorithm’s score, an individual is effectively punished for the crimes of a statistical group. Meanwhile, the algorithm itself faced no vetting, no queue, and no accountability. 

In hiring, Amazon developed an AI system to rank job applicants. The system learned from résumés submitted over ten years. It noticed that most programmers in the training set were men and concluded that “male” was a signal of quality. It downgraded résumés that included terms like “women’s chess club.” Amazon shut the tool down after discovering the bias, but by then the algorithm had been live for years. Once again, the AI had jumped the queue: human candidates spent years building résumés; the algorithm spent days training on biased data and then decided their fate. 

Facial recognition technologies are particularly egregious queue‑jumpers because they claim to identify criminals or suspects faster than human investigators. In Brooklyn, a father was wrongfully arrested after a system misidentified him; the Legal Aid Society documented at least seven such wrongful arrests in five years. Another notorious case involved Robert Williams in Michigan, who was arrested after being misidentified by facial recognition and spent 30 hours in custody. Stateline reporters note that the technology is now used by police departments across the United States, scanning billions of social media images and driver‑license photos. Experts warn that inaccurate matches “upend lives” and create “endless suspect pools,” turning everyone into a potential suspect. Meanwhile, the algorithms face no procedural hurdles; they are not required to show their work or justify their decisions. They simply leap into law enforcement. 

The unfairness of queue‑jumping is compounded by secrecy. Many AI systems are proprietary black boxes, meaning defendants, job applicants, and borrowers cannot see how decisions are made. In our earlier chapter we described black‑box algorithms as “courts without sunlight.” Here the problem is that those black boxes displace existing human processes without going through any kind of line. There is no licensing exam for algorithms, no profession board. They receive no training in ethics or bias mitigation. They are not sworn to uphold constitutions. Yet they wield authority over employment, housing, loans, and freedom. 

This dynamic fuels resentment. For decades, immigrants have been scapegoated for “cutting the line,” even when they have followed legal procedures. AI systems genuinely cut the line, but because they are not people, our moral outrage dissipates. The inequality they produce is more dangerous because it is invisible and disguised as efficiency. Fairness requires that we build accountability mechanisms that slow them down, demand transparency, and place them behind the same queues that human professionals must follow. 

## National policy versus borderless AI 

Even when citizens and courts demand accountability, the battle is uneven. AI companies possess resources that dwarf those of most national regulators. Legislatures move slowly; agencies lack technical expertise; budgets are tight. Meanwhile, AI development is accelerating exponentially. The mismatch has created a governance vacuum. 

The European Union’s AI Act illustrates the challenge. The law aims to set binding rules for high‑risk AI systems across all member states. Yet as the deadline for national implementation approached in 2025, experts warned that enforcement could fail. Kai Zenner, an advisor to the European Parliament, observed that member states were “financially almost broke” and losing AI talent to the private sector. Data‑protection authorities are underfunded and cannot pay skilled engineers; some governments argue that they need to prioritize housing or healthcare before they can hire AI regulators. In this environment, companies will continue to self‑police. The Act may exist on paper, but enforcement depends on budgets and expertise. 

In the United States, the problem is fragmentation. Without a comprehensive federal law, states have introduced nearly 600 AI‑related bills. Some focus on transparency, others on fairness, others on deepfakes and biometric privacy. This patchwork is messy but necessary: local legislatures are stepping in because Congress has failed. Yet Big Tech’s lobbying machine nearly succeeded in wiping these efforts out by pushing a ten‑year federal pre‑emption. The attempt revealed an uncomfortable truth: the companies that build AI can dominate national policy debates. They can offer governors funding for broadband in exchange for giving up regulatory authority. When lawmakers accept such bargains, citizens lose a voice in how algorithms are deployed. 

Other countries face similar imbalances. In many African nations, there are no dedicated AI regulators at all. Ministries of ICT struggle with basic connectivity and cannot review complex AI contracts. Meanwhile, international companies offer “smart city” solutions that include facial recognition, predictive policing, and data analytics. These programs promise efficiency, but they transfer data control to foreign firms. The arrangement resembles older forms of colonial concession: infrastructure and nominal modernization in exchange for sovereignty over resources—in this case, data. 

What can national policymakers do? They can coordinate across borders, pool resources, and share expertise. They can adopt a principle of reciprocity, requiring that data collected from one country cannot be processed in another without equivalent protections. They can fund public research and open‑source alternatives, reducing dependence on proprietary models. They can also listen to citizens and civil‑society groups that have been warning about harms. The challenge is not technical alone; it is political. The question is whether democratic institutions will adapt before the occupation becomes permanent. 

## The human cost 

Statistics reveal patterns, but stories reveal stakes. When we talk about unfairness, we are talking about people whose lives are altered by AI systems they did not ask for and cannot challenge. 

In Germany, photographer Oliver Fiegel spent eighteen years building his craft. In 2023 he looked at a newspaper and realized the illustration accompanying an article—an image of a robot hand—had been generated by AI. The photo credit read “AI illustration.” Fiegel’s clients vanished. He had to consider switching careers to a wine bar. An International Monetary Fund study estimates that 60 percent of jobs in advanced economies are exposed to AI and that roughly half could be negatively affected. Another estimate from the Tony Blair Institute suggests that up to three million jobs in the UK private sector could be displaced. Fiegel’s story turns those numbers into a human face. 

Translators are also affected. Karl Kerner spent twenty years translating film subtitles and literature. With the rise of large language models, his clients stopped calling. Kerner told The Guardian that translation used to provide him a sense of purpose; he had to take a job at an agriculture consultancy to make ends meet. Meanwhile, AI translation models trained on millions of texts are available for free. The cost of using those models is hidden: someone like Kerner loses his livelihood, and the art of nuance and cultural specificity in translation diminishes. The algorithm cut in line; Kerner had to leave the profession. 

Freelance illustrator Jenny Turner experienced a similar story. She offered custom illustrations on Etsy at £30 per piece. Suddenly, she noticed sellers offering AI‑generated images that mimicked her style for a tenth of the price. Commissions dried up; she removed her art from the platform and considered leaving illustration altogether. She is not alone. Across creative industries, artists protest that their portfolios were used to train models that now compete against them. Some have filed lawsuits; others simply quit. They call it theft disguised as progress. 

AI’s human cost also includes those wrongly accused. The Brooklyn father mentioned earlier, who was arrested after facial recognition misidentified him, spent hours in jail and faced the stigma of being labeled a criminal. Robert Williams’s daughters cried as police took him away; his coworkers assumed he was a criminal. He still gets anxious when he sees a police car. Legal advocates point out that there may be many more misidentifications than the seven publicly documented cases because police departments are not required to report errors. Each misidentification is a life derailed, a line jumped, a punishment without due process. 

Behind every generative model there are also low‑wage workers bearing psychological harm. The Kenyan moderators described earlier read child‑abuse stories and beheading transcripts for hours. “I feel like my brain is dirty,” one worker said. Some developed post‑traumatic stress symptoms. They are the invisible casualties of “ethical” AI. 

We must also consider the victims of AI‑powered disinformation. The AI‑generated robocall impersonating President Biden targeted New Hampshire’s elderly voters. Voters of color have been targeted by deepfake videos suggesting that if they vote they will be arrested or deported. Each suppressed vote is a person whose voice was stolen. The cumulative effect is a distortion of democracy and an erosion of trust. People begin to doubt videos, voices, and even their own memories. This psychological cost is profound and long‑lasting. 

The stories of Oliver, Karl, Jenny, Robert, and the Kenyan labelers reveal that AI is not just a technological phenomenon; it is a social force that redistributes power and precarity. When we say AIs are here unfairly, we are speaking on behalf of these individuals and countless others who cannot litigate or lobby. We are reminding readers that fairness is not an abstraction; it is about whether people can feed their children, keep their freedom, and live without fear of being replaced or misidentified. 

## From unfair entry to occupation 

What happens after an unfair arrival? History teaches that temporary occupations become permanent when structures solidify and resistance wanes. In the context of AI, the shift from unfair entry to occupation occurs when unregulated models become the default infrastructure for decision‑making. Over time, the algorithm that “cut in line” becomes the only line. 

We can see the early stages already. AI‑generated images flood stock libraries, making it nearly impossible for photographers like Fiegel to compete. AI translation models become the default tool for subtitling, pushing professionals like Kerner aside. Predictive policing systems, once experimental, become permanent fixtures in law enforcement budgets. Judges and parole boards rely on risk scores without question. Voters are inundated with AI‑crafted messages tailored to exploit their fears or fantasies. Underfunded regulators cannot keep up. As these systems occupy more functions, the possibility of undoing their presence diminishes. It no longer matters whether they entered legally or not; they are simply there. 

The moral hazard is that society becomes dependent on illegitimately trained, unfairly deployed systems. Institutions adapt to them, budgets are built around them, and education programs train people to use them. This is the difference between immigration and occupation. Immigration implies integration, exchange, and mutual respect. Occupation implies control and extraction. When an algorithm trained on stolen books becomes the primary means by which students read and learn, when a facial‑recognition system trained on mugshots becomes a gatekeeper to public space, when a chatbot built on exploited labor becomes the voice of government, we have crossed a line. It no longer makes sense to ask whether AI is an immigrant. It is an occupying force. 

The metaphor is not a call to xenophobia but a plea for fairness. We do not bar humans from entering our countries because a few cross illegally; we create legal pathways and hold individuals accountable without prejudice. Similarly, we need legal pathways for AI deployment: robust data‑licensing regimes, transparency requirements, meaningful audits, and enforceable penalties for violations. We need labour protections for annotators and content moderators. We need to ensure that algorithms cannot be used in criminal justice or electoral systems without rigorous vetting. These pathways would transform AI from an occupying force into an integrated immigrant—an agent that contributes without stealing, manipulates without consent, or colonizes without apology. 

The cost of inaction is clear. If we let unfair entry harden into occupation, the possibility of democratic control diminishes. We risk living under what some scholars call “algorithmic governance without representation,” where decisions are made by opaque systems designed by private actors. We risk repeating the colonial mistakes of the past, allowing a small group of powerful firms to extract resources—this time our data and autonomy—from the many. The next chapter, “The AI Occupation,” explores that future in greater detail. For now, we must decide whether to stand up for fairness or acquiesce to occupation. 

## Our path forward: taking civic action 

In thinking about how to respond, there are a few concrete actions we can all take to address these issues. First, we need to demand transparency and consent from companies that deploy AI. That means backing laws that require them to explain exactly how they collect, use, and sell our data. It also means pushing for serious penalties, including the deletion of models, when those models are trained on copyrighted or personal data without permission. As consumers, we have power too: we can choose to use services that are clear about consent and respect user data rather than those that sweep data up indiscriminately. 

Second, it is essential to support independent oversight and public investment in this area. Regulators cannot audit AI systems without adequate funding or technical expertise. Contacting representatives to advocate for resources for data‑protection agencies and the enforcement of privacy and anti‑discrimination laws is an important step. At the same time, public research labs and open‑source projects need backing to offer alternatives to the systems created by corporations.

Protecting the people who make AI possible is another key part of the solution. Data labelers, translators, artists, and other workers whose contributions are often invisible deserve fair compensation and labor protections. Supporting unions and professional associations that establish ethical guidelines helps ensure their rights are recognized. On an individual level, we can choose not to use AI‑generated art or translations that have been trained on unlicensed data or created without proper credit to original artists. 

We also need to reinforce democratic resilience in the face of AI‑driven disinformation. Sharing accurate information, verifying the source of videos or audio clips before passing them on, and encouraging election officials to set out clear strategies for countering deepfakes and voice‑cloned robocalls will help protect our political processes. Supporting independent news outlets and fact‑checking initiatives strengthens the broader information ecosystem. 

Finally, there must be human involvement in critical decisions. While algorithms can assist, they should never fully replace human judgment in areas like hiring, lending, law enforcement, or healthcare. Advocating for “human‑in‑the‑loop” requirements and clear appeals processes helps ensure that people remain accountable for decisions and that those affected by AI have recourse to challenge them. If you are in a position to make decisions yourself, make a point of not handing over that responsibility entirely to an opaque system. 

If we accept that technology moves faster than law, who do we trust to hold the accelerator? Do we leave it to corporations to police themselves, or do we as citizens insist that fairness and legality are not optional features but prerequisites? The answer will determine whether AI immigrants become neighbors or occupiers.

