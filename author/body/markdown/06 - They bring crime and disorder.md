# They bring crime and disorder 

They come without passports and leave with your identity.

Every immigration wave brings fears that the newcomers will erode public safety. In the nineteenth century it was Irish dock‑workers; in the twenty‑first it is families crossing an ocean for refuge. Today the figure of the dangerous outsider has taken a digital form. Artificial intelligences – the models that suggest songs, reply to emails and generate artwork – are also lawless migrants from a wild frontier. The presence of these “algoes” does make us less secure, they make crime easier and our institutions chaotic. The chapter that follows explains how AI tools can be weaponised to amplify crime and disorder, not because they are malicious themselves, but because human actors exploit their capabilities. The aim is not to provoke panic. Instead, it offers a clear‑eyed account and asks what citizens and governments can do

## Fear of disorder and the immigrant metaphor 

The immigrant metaphor that frames this book is not an accusation against machines; it is a lens on our society. When people worry that a group “brings crime,” they are often projecting deeper anxieties about economic insecurity and social change. The same is true of AI. Criminals and authoritarian regimes have already harnessed automation to commit fraud, suppress dissent and infiltrate systems. But AI itself is a tool, akin to a car or a printing press. Blaming the tool alone obscures the human agency that decides whether it is used to help or to harm. In this chapter we will distinguish between intrinsic risks (the ways an algorithm can behave unexpectedly) and misuse (deliberate exploitation by bad actors). The comparison to immigrants also highlights how generalised fear can lead to discriminatory responses. Rather than panic, we need policy and oversight. The next sections examine different kinds of harm and show that behind the headlines there are human decisions, corporate incentives and regulatory gaps. 

## Human rights violations and crimes against humanity 

One of the most chilling applications of AI is its use by governments to monitor, predict and control populations. Investigations by the European Parliament into the Xinjiang region of China reveal that authorities there use AI‑driven predictive policing and facial recognition to identify and detain members of the Uyghur minority. Data from cameras, license plate readers and mobile devices are fused in systems like the “Sharp Eyes” programme to identify “focus personnel” and flag behaviours deemed risky. These data are fed into the Integrated Joint Operations Platform, which links ID numbers to physical characteristics and uses machine learning to generate lists of individuals for detention. Many Uyghurs have been sent to re‑education camps without charges or trials, purely on the basis of algorithmic predictions. The system’s designers claim it prevents terrorism; in practice it enables the mass detention of an ethnic group. 

Predictive policing has also been tried in democratic societies, often with troubling results. A 2024 report from the U.S. Department of Justice notes that many predictive models rely on historical arrest data that reflect human biases. Tools trained on such data tend to over‑police Black and Latino neighbourhoods and misidentify individuals. Some agencies have abandoned person‑based predictive policing because it offers little value while posing privacy risks. The same report warns that risk assessment algorithms used in courts can perpetuate inequality by categorising people differently despite similar histories. At least seven mistaken arrests in the United States have been linked to facial recognition errors, nearly all involving Black citizens. Here, the harm is not that AI “decided” to target minorities; it is that police departments adopted commercial systems without adequate testing or oversight, and built them on data sets saturated with systemic bias. 

To understand why such abuses flourish, one must look at corporate incentives. Companies selling surveillance and predictive tools market them as neutral, efficient and profitable. Governments, eager to demonstrate security, purchase them. Ordinary citizens – often those with the least power – become the test subjects. This dynamic mirrors xenophobic narratives about immigrants as criminals: the focus is on control, not accountability. Recognising that AI tools can be weaponised for oppression helps us demand better protections. International human rights law already forbids arbitrary detention and discrimination; those norms must be enforced when algorithms are involved. Transparency about data sources and auditing of systems can help. Importantly, the victims of algorithmic repression are not anonymous masses but individuals with families, livelihoods and communities. Their stories remind us that “crime and disorder” can be inflicted by states as well as by thieves. 

## Copyright expropriation and creative theft 

Another area where fears of AI echo familiar immigrant stories is intellectual property. Creators worry that outsiders will steal their work and erode their livelihoods. In the context of AI, those fears are grounded in reality. Large language and image models are trained on vast corpora of text, art and music scraped from the internet, much of it copyrighted. A federal case decided in June 2025 illustrates the tension. Authors including Sarah Silverman and Ta‑Nehisi Coates sued Meta, alleging that the company used pirated versions of their books to train its model Llama. U.S. District Judge Vince Chhabria dismissed the case, stating that the plaintiffs had not provided enough evidence that Meta’s use would dilute the market for their work. But he also emphasised that using copyrighted work without permission would be unlawful in “many circumstances” and that this ruling simply reflected poorly argued claims. Chhabria noted that generative AI could flood markets with endless images, songs and articles created with a fraction of the effort humans expend. In other words, the court recognised that models trained on copyrighted works may undermine incentives for creation. 

The deeper problem is opacity. Training datasets are often poorly documented and inconsistently licensed. A 2025 analysis from MIT Sloan observes that popular models like GPT‑4 are built on data whose lineage is unclear. Without transparency, developers and users may unknowingly violate regulations like the European Union’s AI Act or expose themselves to copyright lawsuits. Researchers with the Data Provenance Initiative audited more than 1,800 text datasets and found that license information was omitted in over 70 % of cases, while license types were miscategorised over 50 % of the time. These errors make it hard for responsible companies to select lawful training data, and they make it easy for unscrupulous actors to claim ignorance. 

Artists and journalists describe how their work is cannibalised. A painter might find derivative images flooding social media, produced by an AI trained on her portfolio. A novelist’s style can be imitated to generate endless fan fiction. When newspapers discovered that a model reproduced their paywalled articles verbatim, they sued. The question is not whether AI should be allowed to learn from human culture – that is how humans learn – but whether those whose work feeds the models should have a say and receive compensation. Historically, immigrants have been accused of stealing jobs and culture while in reality they enrich and diversify societies. The same is true of AI if governed wisely: it can inspire new art and collaboration. Policies requiring dataset provenance, consent from creators and fair remuneration would reduce the feeling of expropriation and align incentives toward creativity rather than theft. 

## AI-powered scams and social engineering 

Fraudsters have always exploited new technologies. The telephone enabled “grandparent scams” and phishing emails promised fake inheritances. AI lowers the cost and raises the realism of such tricks. In early 2024 the Electronic Frontier Foundation warned about voice-cloning scams. Criminals record a few seconds of a person’s voice – from social media posts or voicemail greetings – and then generate convincing audio messages. One scenario recounted by investigators begins with a panicked call: a voice that sounds like your child says they have been arrested and need money. A second voice posing as a lawyer demands immediate payment. The voices are realistic because websites now offer cheap voice‑cloning services; in some cases scammers need only a thirty‑second audio sample. Victims often wire money before they verify the situation. The EFF notes that while such scams are not yet widespread, the technology is accessible enough that they could become common. They recommend simple defences such as family passphrases. 

Corporate scams have already moved beyond prototypes. In 2024 an employee at the engineering firm Arup in Hong Kong joined a video call with people who looked and sounded like senior executives. He believed he was fulfilling a routine transfer, but the meeting was a deepfake. Criminals used AI‑generated video and voice to impersonate multiple executives simultaneously and convinced him to transfer HK$ 200 million (about £20 million) into accounts they controlled. The case shows that deepfake scams are no longer restricted to simple audio; they can involve coordinated video conferences, realistic backgrounds and body movements. Arup’s chief information officer told reporters that attacks including invoice fraud, WhatsApp voice spoofing and AI‑generated deepfakes have “risen sharply”. Law enforcement reported no arrests. 

The problem is not only deception but also scale. AI models can create thousands of tailored phishing emails that mimic a company’s writing style and include plausible personal details gleaned from social media. Cybersecurity experts warn that generative models can automate phone scripts, design fake websites and simulate entire call‑centre operations. Once criminals used to need technical skills to craft malware; now they can ask a model to write ransomware or produce a PDF laced with malicious code. The victims are not just large companies. A small bakery owner in our town of Rugeley described receiving an email that appeared to be from her supplier; it contained a new bank account number and a friendly note referencing a recent conversation. She wired £8,000 before realising she had been scammed. She later learned that the attackers had used AI to scrape her posts and invoice templates, making their request familiar and credible. In this way AI democratises fraud: both criminals and victims are everyday people. 

## AI-assisted hacking and cybercrime 

The same capabilities that allow AI to compose poetry can be redirected to composing malware. A 2025 threat intelligence report from OpenAI describes how its investigators detected and disrupted several campaigns in which state‑linked actors used generative models to enhance cyberattacks. In one case, North Korean operators used ChatGPT to generate résumés and cover letters at scale to infiltrate IT companies with deceptive job applications. They even sought advice on remote‑work setups and used AI‑generated content to recruit collaborators. The same report documents Chinese and Russian operations that used AI to craft social‑engineering scripts, spam comments and disinformation. OpenAI notes that AI can be used at every stage of an attack: reconnaissance (asking about software vulnerabilities), development (writing and debugging malware), phishing (crafting credible emails) and post‑exploitation (automating network scans). The company says its systems help detect and shut down such misuse, but the broader security community must adapt. 

Generative AI also reduces skill barriers for hackers. The BleepingComputer analysis of OpenAI’s February 2025 disruption efforts found that threat actors used models to ask about default passwords, identify configuration errors, and obfuscate malicious code. Attackers who previously relied on publicly available scripts can now generate customised payloads and adapt them to specific networks. Combined with stolen credentials from massive data breaches, these tools enable “credential stuffing” on an industrial scale. The Identity Theft Resource Center (ITRC) reports that in the first half of 2025 there were 1,732 data compromises, an 11 % increase over the same period in 2024. Cyberattacks accounted for 77.8 % of these incidents, compromising the personal data of more than 114 million individuals. Phishing and business‑email compromise were the second most common cause, followed by ransomware. The ITRC notes that generative AI platforms are increasingly used to create sophisticated phishing campaigns that are harder for organisations and individuals to detect. 

These trends create what cybersecurity scholar Qi Liao calls “ransomware 2.0”: attacks that not only encrypt data but also steal and sell it. Liao explains that AI can automate and personalise phishing by analysing social‑media posts, generate deepfakes for blackmail, and craft malware that evades detection. He warns that adversarial machine learning techniques can bypass spam filters and anti‑virus systems. Meanwhile, criminals can cross‑reference anonymised data from medical, financial and voter records to re‑identify individuals and learn their habits. It is a chilling inversion of AI’s promise: tools meant to secure and streamline our lives are used to break into them. 

## Identity theft and the erosion of privacy 

Identity theft is the crime that best illustrates the intersection of AI, data and everyday life. In 2025 the UK fraud prevention service Cifas reported that more than 217,000 fraud risk cases were logged in the first half of the year, a record number. Of these, over 118,000 were identity fraud cases. Cifas warns that AI enables criminals to create synthetic identities: fabricated personas that combine stolen data with invented attributes to bypass verification systems. Economic pressures and risky consumer behaviour mean that some individuals even sell their own identities, exposing themselves to liability when criminals take loans or open accounts in their names. Sectors once considered low risk, such as motor insurance and telecommunications, are now prime targets. Facility takeovers – where attackers seize control of existing bank or phone accounts – accounted for 18 % of all fraud, and the telecommunications sector saw the steepest rise. 

The ITRC adds that data breaches feed these scams. Among the 165 million individuals affected by breaches in the first half of 2025, many had their login credentials and personal identifiers packaged into a single database. This repackaging means that criminals can easily buy lists of usernames and passwords and use AI to test them across multiple services, a tactic known as credential stuffing. James E. Lee of the ITRC notes that more than two‑thirds of breach notifications fail to disclose the attack vector, leaving consumers unaware of how their data were stolen. The lack of transparency prevents individuals from taking targeted protective steps and undermines public trust. 

Beyond financial fraud, privacy erosion occurs through de‑anonymisation. AI can cross‑reference public data with stolen datasets to identify individuals in supposedly anonymous datasets. For example, a malicious actor could combine a health‑care dataset with voter registration information and smart‑home device logs to infer who is being treated for a particular condition and when they are away from home. Such granular profiling enables blackmail, targeted advertising and discrimination. The victims might never know how they were selected for a scam or denied a job. If data is “the new oil,” then identity theft is the new pipeline leak – invisible until a spill occurs. 

## Algorithmic coercion into violence and self‑harm 

While AI is often blamed for making people do things, the reality is subtler. Recommendation systems are designed to maximise engagement, and they do so by amplifying content that elicits strong emotions. Sometimes that content includes hate, extremism and self‑harm. A 2024 study of Instagram’s moderation revealed that Meta failed to remove any of 85 posts showing various degrees of self‑harm. Even more troubling, Instagram’s algorithm encouraged accounts posting self‑harm images to befriend each other, helping a network of harmful content grow. Psychologist Lotte Rubæk told reporters that such failures trigger vulnerable users and can be a matter of life and death. Rather than accidental oversights, these outcomes reflect an incentive structure that rewards engagement over safety. 

Scholarship on extremism shows a more complicated picture than popular accounts suggest. In a review of empirical studies, communication researcher Aaron Shaw notes that while public narratives often blame recommendation algorithms for radicalizing unsuspecting viewers, the evidence is less clear-cut. YouTube’s algorithm rarely pushes extremist content to casual users. However, the platform still hosts vibrant extremist communities, and its design can help reinforce the views of those already inclined toward resentment. Many radicalized individuals reach these networks through external links or by subscribing to fringe channels rather than stumbling upon them by accident. Once inside, gaps and inconsistencies in moderation allow conspiracies, misogyny, and hate to circulate with relative freedom. Researchers have linked participation in these online spaces not only to hardened attitudes but also to higher likelihoods of involvement in civil unrest and, in some cases, political violence. 

There is also evidence that algorithms can overexpose vulnerable people to self‑harm content. A 2025 qualitative study with young people, policy makers and social‑media industry professionals found that 83 % of youth have encountered self‑harm or suicide content online, often before the age of fourteen. Participants warned that the way social‑media algorithms operate can lead to overexposure when teens interact with peers. Despite these risks, national suicide‑prevention strategies rarely include social media guidelines. Social‑media companies have adopted some policies, but they differ across platforms and lack independent oversight. The result is a patchwork of private rules that leaves many gaps. When tragedies occur, companies often say “we didn’t design it to do that.” That refrain echoes the way some societies blame immigrants for social ills without examining the policies and incentives that create the conditions for exploitation. 

## Opacity, accountability and the black box problem 

Underlying many of these harms is the black box nature of AI systems and the secrecy of their creators. The MIT Sloan report mentioned earlier points out that without transparency into the lineage of training data, it is difficult to assess legal risk or bias. The Data Provenance Initiative discovered that more than half of licensing information in popular datasets was miscategorised and that over 70 % of datasets lacked license information. These findings show that many companies build products on shaky foundations. 

Opacity is also present in the public sector. During the first half of 2025, the ITRC noted that 69 % of data‑breach notifications failed to disclose how the breach occurred. This means that victims are left without information that could help them respond, and patterns of attack remain hidden. Similarly, when algorithms misidentify individuals or recommend harmful content, companies often claim trade secrets to avoid revealing how their systems work. Such secrecy hinders redress and allows abuses to continue. In a 2024 Mozilla Foundation report examining watermarking and labelling of AI‑generated content, researchers concluded that human‑facing labels are easy to manipulate and can even erode trust. They argue for machine‑readable markers combined with robust detection mechanisms and a multifaceted governance approach. Without strong disclosure requirements, citizens cannot know whether a video or call is authentic. 

The black box problem is exacerbated by corporate incentives. Proprietary models are guarded to maintain a competitive edge, even when they have public consequences. This is akin to a landlord refusing to repair a building because the blueprint is a trade secret. Regulation must strike a balance between protecting innovation and protecting the public. The California Generative AI: Training Data Transparency Act, signed in 2024, is an example of legislation that requires companies to document their training data sources. European policymakers are considering similar measures. Transparency alone is not a panacea – after all, criminals will not volunteer their methods – but public institutions have a duty to set standards. 

## Agency, law and remedies 

The narrative that AI immigrants bring crime and disorder can lead to two dangerous responses: complacency (“nothing can be done”) or technophobic bans. Both ignore the role of human agency. The harms described above stem from human decisions to deploy surveillance against minorities, build opaque models on stolen data, design engagement‑maximising algorithms without safety checks, and weaponise generative tools for fraud. Human decisions can also mitigate those harms. 

Strengthening civil liability is the first step toward accountability. Just as landlords are responsible for safe buildings, companies that deploy AI should be liable for foreseeable harms. If a predictive policing tool leads to unlawful detention, those affected deserve the right to sue and demand audits. Courts could require disclosure of model assumptions and training data, while whistleblower protections should extend to engineers and ethicists who raise concerns about misuse.

Alongside liability comes the need for data-provenance requirements and licensing. Policymakers can require companies to keep records of where training data come from, what licenses apply, and how data subjects can opt out. The Data Provenance Initiative has already shown this is feasible—auditing datasets, reducing unspecified licenses from 70 % to 30 %, and offering tools for exploration. Clear provenance not only supports copyright holders but also protects companies from ingesting harmful or illegal material, like the child abuse images found in the LAION-5B dataset.

Any serious framework also demands mandatory red-team audits. Before deployment, systems should undergo adversarial testing by independent researchers to expose vulnerabilities and misuse pathways. The practice is standard in cybersecurity and can be adapted for AI. Red teams should bring together experts in human rights, mental health, and cybersecurity, not only technical engineers. Their findings ought to be published, fixes documented, and lessons shared.

Education and public infrastructure matter just as much as oversight. Citizens need accessible information about deepfake scams, identity theft, and privacy safeguards. Public-awareness campaigns—akin to anti-smoking or seat-belt initiatives—could help. Simple practices like setting family passphrases, verifying calls through known numbers, using password managers, and enabling two-factor authentication can make a huge difference. Public libraries and community centres could act as hubs for digital-literacy workshops.

Governance at the platform level is equally vital. Social-media companies should grant researchers access to data, with privacy safeguards in place, so the societal impact of algorithms can be studied properly. Laws such as the UK’s Online Safety Act or the proposed U.S. Kids Online Safety Act should include explicit provisions on suicide and self-harm content. Algorithms that recommend content need built-in throttles for harmful material and should be subject to external review.

All of these measures point toward a broader truth: responsibility is collective. Crime and disorder don’t arise because tools are inherently evil; they arise because humans choose to exploit them. Casting AI as an invading criminal class risks repeating old patterns of scapegoating. A more constructive view treats AI as a new population of residents in our digital society. Like any residents, they need rules, oversight, and a culture of care. Our task is to shape those rules so that AI contributes to safety and justice, rather than undermining them.

## From fear to agency

If the previous chapters have shown that AI immigrants do not always integrate or always work, this chapter confronts the fear that they will bring crime and disorder. The evidence is nuanced. Yes, AI can be weaponised for repression, fraud and hacking. Yes, algorithms can amplify self‑harm and extremism. Yes, opaque practices make accountability difficult. But at every turn, human decisions, incentives and institutions shape those outcomes. Understanding the mechanisms of harm – predictive policing built on biased data, datasets scraped without consent, algorithms designed to maximise engagement – empowers us to change them. 

As you finish this chapter, consider the parallels between today’s AI and yesterday’s immigrants. Demonising a group rarely leads to safety; building equitable systems does. What role can you play, as a voter, consumer or creator, in demanding transparency, accountability and care from those who build and deploy these powerful tools? 

