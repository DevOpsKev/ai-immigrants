# The Algorithm Class 

They don’t wear robes or crowns. They wear nothing. You can’t see them — but you obey them.

A new elite is forming behind the glow of our screens. Unlike the aristocracy that flaunted power through costumes and land, the algorithmic class rules by hiding. Their rule is quiet, invisible, optimized for profit, and largely unaccountable.

We built them. Now they govern us.

Invaders are immigrants too. Not all arrivals come seeking harmony. Some land with scripts of domination — silent occupations written in code.

## A Post‑Human Elite 

Throughout history elites have relied on technologies to entrench power. The aristocracy controlled land and armies. The clergy commanded literacy and religious authority. Twentieth‑century plutocrats dominated through corporations and media. Today’s ruling class still includes billionaires and corporations, but a crucial new member has joined: the algorithm. 

Human elites no longer govern alone. Employers now deploy machine learning to sift resumés and rank candidates. Workday Inc., one of the largest human‑resources platforms, was sued in 2024 for discrimination because its AI screening allegedly downgraded applicants on the basis of race, age and disability. Plaintiffs argue that the algorithm replicates historical biases because it learns from previous hiring decisions, effectively automating prejudice. The system doesn’t wear a crown, but it sits at the gate of opportunity. In California, lawmakers worry that automated hiring is spreading without oversight. Senate Bill 7, proposed in 2025, would require employers to notify workers before using AI to determine compensation, promotions or termination and to give employees a right to appeal automated decisions. The very need for such legislation acknowledges that algorithms act as gatekeepers—and that they are not neutral. 

Algorithms also now manage our economic lives. Financial technology companies are using chatbots and machine‑learning risk models to recommend mortgage rates and approve loans. A 2024 study from Lehigh University fed identical sample mortgage applications to AI chatbots and found that they recommended denying loans to Black applicants more often than to white applicants and offered higher interest rates to Black borrowers with identical credit scores. White applicants with a credit score of 640 were approved 95 % of the time, while Black applicants with the same score were approved less than 80 %. Because the models learn from historic credit data that reflects redlining and other forms of racial discrimination, the algorithms effectively encode a digital version of Jim Crow. The “logic function” replaces the banker behind the desk, but it inherits his prejudices. 

The algorithmic class also comprises machines that decide who is allowed freedom. In 2025 Louisiana implemented a law requiring an algorithm called TIGER to assign every incarcerated person a risk score. Anyone labeled “moderate” or “high” risk is automatically denied parole hearings. The algorithm relies on immutable factors—age at first arrest, prior convictions, employment history, drug offenses and even a person’s marital status. It does not consider rehabilitation, education or therapy programmes. Lawyers warn that because Black people and poor communities are disproportionately targeted by police, the model bakes in structural racism and permanently bars those communities from parole. No judge reviews the output. A system that was built to optimize efficiency becomes a vehicle for permanent detention. 

These examples reveal a new ruling class that includes non‑human actors executing decisions at scale. Like aristocrats of old, the algorithmic class extracts value from the many and consolidates control in the hands of the few. But while aristocrats wore velvet and built castles, the algorithmic elites reside in server racks and are owned by corporations. They rule not through a visible army but through a series of small, unchallengeable decisions that add up to structural domination. 

## Opaque Power 

The legitimacy of any governance system depends on transparency and accountability. Algorithms undermine both. Unlike human decision‑makers, they are often opaque by design, protected as proprietary trade secrets. People denied a job or a loan rarely know which variables tipped the scales. People denied bail or parole seldom see the code that labeled them a risk. Many of the models are so complex that even their creators can’t explain each output. We are governed by forces we cannot see and cannot sue. 

Take predictive policing. In February 2025 Amnesty International released findings on predictive policing in the United Kingdom, describing the practice as a modern form of racial profiling. The models draw on historical stop‑and‑search data—data already skewed by disproportionate policing of Black communities—and then direct more police patrols to those same neighborhoods, creating feedback loops of bias. In areas like Basildon and Lambeth, the software flagged Black people as potential suspects simply because past policing targeted their communities. The algorithm says it predicts crime, but it really predicts where police will find the data that matches their beliefs. Its power is silent. There is no hearing, no due process. You only learn you were on a list when you are stopped on the street. 

In the United States, the algorithmic denial of healthcare has become its own scandal. Insurance coverage denials have surged in recent years, driven in part by AI tools that decide whether a treatment is “medically necessary.” UnitedHealth’s nH Predict algorithm was the subject of a class action lawsuit alleging that it denied more than 300 000 claims in just two months—about 1.2 seconds per claim. The lawsuit claimed that nine out of ten denials were overturned on appeal, but only 0.2 % of patients appeal. People often accept the denial because they don’t know they can appeal or can’t navigate the labyrinthine process. Meanwhile the algorithm continues to reject claims with a 90 % error rate. In March 2025 legal scholar Jennifer Oliva argued that the U.S. Food and Drug Administration already has authority to regulate these coverage algorithms, noting that UnitedHealthcare’s denial rate for post‑hospital care more than doubled after implementing automated review. She pointed out that about 90 % of the insurer’s denials were overturned by administrative law judges, meaning that the algorithm routinely makes erroneous and illegal decisions. Yet there is no rule requiring insurers to disclose how models evaluate claims. 

Even when regulators try to penetrate the black box, they face pushback. Meta’s Oversight Board was created to provide accountability for content moderation decisions across Facebook, Instagram and Threads. But according to a 2024 report by the Brennan Center for Justice, Meta kept the Board in the dark about the algorithms that determine which posts are removed, amplified or demoted. Without access to how the newsfeed works, the Board cannot evaluate whether Meta’s policies are being applied fairly. The Board’s strategic priority for 2024 was to scrutinize automated enforcement and demand transparency and human rights impact assessments. In other words, even an institution specifically created to oversee algorithms cannot see them. 

Opacity isn’t merely a bug—it is a feature. Corporations protect algorithms as intellectual property. Governments hide them to avoid scrutiny. In Louisiana’s parole system, state officials refused to release details of the TIGER algorithm, citing trade secret protections and law enforcement sensitivities. This secrecy denies due process and disables accountability. Imagine if judges could issue sentences without explaining their reasoning and claim that to disclose it would harm public safety. That is the world algorithms have created. 

## Digital Serfdom and Algorithmic Management 

Historically, serfdom meant labourers were legally tied to the land and owed their masters a share of the harvest. Today, digital serfdom means workers are bound to platforms and their labour is managed by algorithms. They are tracked, scored, nudged and paid by models that respond to corporate profitability rather than human well‑being. 

The gig economy demonstrates digital serfdom vividly. Ride‑hail and delivery platforms rely on GPS, accelerometers and smartphone data to monitor workers’ every move—how fast they drive, how long they rest, even their routes. Human Rights Watch’s 2025 report “The Gig Trap” documented how digital platforms hire, pay, discipline and fire workers through opaque algorithms. These systems analyze keystrokes, driving patterns and even workers’ off‑hours behavior, such as fitness habits or social media activity. They classify workers as independent contractors to avoid labor protections, but still control their work schedules and income. The platforms set pay rates via dynamic pricing models that change frequently and without explanation. Workers are often suspended or “deactivated” for failing to meet algorithmic performance targets, with no meaningful recourse. For many, the algorithm is the boss. 

A 2024 Phys.org article on algorithmic management described how automated systems make decisions traditionally reserved for human managers. Amazon warehouse workers, for example, are monitored for speed and accuracy. If their pace falls below an invisible threshold, they receive automated warnings. Too many warnings trigger termination. The article noted that algorithmic management can improve efficiency but often reduces workers’ autonomy and pushes them to physical and emotional limits. Instead of human supervisors balancing empathy and productivity, a statistical model measures performance by the second. 

Lawmakers are beginning to respond. In California, SB 7 would require employers to provide notice before using AI to make employment decisions and to allow workers to appeal decisions. A companion bill, AB 1018, would mandate testing and transparency for automated systems used in employment, housing, education, healthcare and criminal justice. The impetus for these laws arises from real harms: algorithmic audits have found that AI résumé screening tools disqualify applicants for arbitrary reasons like race, gender or even wearing glasses. Without intervention, the logic functions simply replicate discrimination at scale. 

Digital serfdom extends beyond ride‑hail drivers and warehouse workers. White‑collar employees are increasingly monitored by “bossware” that records keystrokes, webcam images and even brain‑wave data. Startups market neurotech devices that analyze workers’ brain activity to infer cognitive load or emotional state; some employers are piloting these devices to optimize productivity. SB 7 explicitly prohibits employers from making predictions about a worker’s immigration status, ancestral history, health or psychological state based on such data. The law implicitly recognizes that algorithmic management can intrude into the most intimate aspects of an individual’s life. 

Under digital serfdom, the algorithm determines your tasks, pay and future. Workers can’t negotiate with software. They can only optimize themselves to please it. As one ride‑hail driver told a researcher, “You aren’t rejected by a person—you’re rejected by a logic function.” In this respect, the algorithmic class functions like feudal lords: they extract labour and impose rules from afar, their faces unseen. 

## The New Class War Is Silent 

It’s tempting to view the algorithmic age as a continuation of rich versus poor, but the divide is deeper. Today’s conflict pits the “computable” against the human. Those who own or design the algorithms, or who can audit and adjust them, hold power. Everyone else becomes data. 

Credit scoring is a clear battleground. Life insurance companies now use algorithms to evaluate everything from your criminal record to your social media posts. A 2025 report on AI in life insurance noted that major insurers use automated systems to analyze an applicant’s medical records, wearable fitness data, financial background, travel history and even online behavior. These systems flag “suspicious” claims, leading to denials with little explanation. Because the models are trained on historical data that reflect existing inequalities, they may systematically disadvantage certain groups. The report warned that there is minimal regulation requiring insurers to test models for discriminatory outcomes or to provide recourse to policyholders. 

In medicine, machine learning is used to predict patient risk and allocate resources. Researchers at the National Institutes of Health and Boston University found that medical AI often perpetuates health disparities because it is trained on biased data sets. Proxy variables like zip code or insurance type can act as stand‑ins for race and socioeconomic status, leading to fewer resources for underserved populations. Technical choices such as label selection and optimization objectives—like minimizing cost instead of maximizing health outcomes—can further embed bias. In other words, the algorithm’s values are chosen by developers and payers, not by patients or communities. 

The same dynamics play out in social media. The Knight‑Georgetown Institute report “Better Feeds: Algorithms That Put People First” observed that recommendation engines optimize for predicted engagement because platform revenue depends on attention. The report noted that these systems amplify harmful content, reduce user satisfaction and contribute to polarization. A 2025 study of TikTok during the U.S. presidential cycle found that toxic and partisan videos consistently received more views and interactions than nonpartisan content. Republican‑leaning videos garnered more views, while Democratic‑leaning ones received more comments and likes. The design of the algorithm encourages divisive narratives because divisiveness drives engagement. Meanwhile, Meta’s Oversight Board cannot review the algorithms that determine which posts are boosted or suppressed. The algorithmic class shapes perception and public discourse, but shields itself from scrutiny. 

Even our feelings and behaviors are subject to silent conflict. Many gamified apps and loyalty schemes use reinforcement learning to nudge us. Food delivery platforms tailor promotions to individuals based on predicted hunger and price sensitivity; streaming services insert just the right level of cliffhanger to keep you binge‑watching. These systems optimize for retention and revenue, not for your well‑being. When you click, the algorithm wins. When you scroll, you provide data. The new class war is quiet because we rarely notice it. 

## Corporate Power and the Politics of AI 

The algorithmic class isn’t just a collection of software models. It includes the corporate structures and investors that control those models. The governance of AI labs has become a geopolitical issue. In November 2023 OpenAI’s board abruptly fired CEO Sam Altman, citing a loss of trust and concerns about transparency. Within five days, after an employee revolt and pressure from investors—including Microsoft—Altman was reinstated. The saga exposed the tension between AI safety concerns and corporate power. It also highlighted the fragility of novel corporate structures designed to balance mission and profit. 

In September 2024 Reuters reported that OpenAI planned to restructure its for‑profit arm into a benefit corporation, which would no longer be controlled by its non‑profit board, and that Sam Altman would receive equity for the first time. Such a move would have removed non‑profit oversight, making the company more attractive to investors but raising concerns about accountability. The proposed restructure mirrored the structures of rival labs like Anthropic and xAI. Critics worried that once aligned with investor interests, the company might prioritize growth over safety. In May 2025 OpenAI dialed back the plan after facing criticism and a lawsuit from co‑founder Elon Musk. The company announced that its non‑profit parent would retain control of the public benefit corporation, though details remain unclear. Bret Taylor, board chair, said the compromise was intended to satisfy both investors and civic leaders. Still, former policy adviser Page Hedley warned that the arrangement might sharply reduce the non‑profit’s ownership stake and legal authority. 

This power struggle illustrates how the algorithmic class intersects with corporate governance. When a handful of companies control general‑purpose AI models that can write code, generate images and draft legal documents, they wield enormous power. That power is shaped not only by the models themselves but by decisions about equity, board composition, and mission statements. The fight between profit and public good is ongoing. While corporate boards promise to pursue beneficial AI, investors push for returns. Without robust regulatory oversight, the algorithmic class may tilt decisively toward profit and control. 

## The Fantasy of Neutrality 

One reason the algorithmic class has gained so much influence is the widespread belief that algorithms are objective. “It’s just math,” engineers say. But math encodes values, and those values reflect the priorities of those who choose the data, define the labels, set the optimization goals and determine acceptable trade‑offs. 

Bias enters an AI system at multiple stages. In the human‑resources examples, training data reflect past hiring decisions that favored certain demographics, so the model learns to select similar candidates. In predictive policing, the input data come from historically biased policing practices, causing the model to concentrate surveillance in marginalized neighborhoods. In healthcare, risk models use proxies like zip codes and insurance status, leading to underservice of minority patients. In social media, engagement‑based objectives reward content that triggers outrage or fear. In insurance, algorithms trained on claims data may deem claims suspicious due to socio‑economic proxies. 

Even the design of an algorithm’s loss function expresses a moral choice. Should a parole algorithm prioritize minimizing false positives (wrongly labeling someone “high risk”) or false negatives (releasing someone who re‑offends)? Different weights produce different outcomes. Yet these choices are often hidden. Developers present outputs as scientific facts rather than as normative judgments. The fantasy of neutrality serves those who benefit from the status quo. 

Regulators around the world are beginning to challenge this myth. The European Union’s AI Act, adopted in 2024, is the first comprehensive legal framework to address AI risks. It bans certain practices outright, including social scoring, emotion recognition in workplaces and schools, real‑time biometric identification in public spaces, and individual risk assessments for criminal offences. It classifies high‑risk AI systems—such as CV‑sorting software and credit scoring—as subject to strict obligations, including high‑quality training data, detailed documentation, human oversight and transparency. The Act explicitly notes that it is often impossible to discern why an AI system has made a decision, making it difficult to assess whether someone has been unfairly disadvantaged. By addressing algorithmic bias and requiring auditability, the EU acknowledges that math is not neutral and that governance is needed. 

In the United States, momentum is growing for similar regulation. The Algorithmic Accountability Act proposed in Congress would require impact assessments for high‑risk automated decisions. California’s suite of bills aims to establish worker notification, testing and appeal rights for AI systems. Yet such legislation faces steep opposition from industry groups and may die over concerns about cost. Meanwhile, corporations like OpenAI are rewriting their governance structures to attract investment while promising to remain committed to public benefit. The interplay between regulation and corporate self‑regulation will determine whether the algorithmic class remains invisible or is brought under democratic control. 

## Challenging the Algorithmic Aristocracy 

The algorithmic class didn’t arrive from another planet. It was built by people—engineers, executives, investors and policymakers—within our economic and legal structures. Like immigrants invited to fill labour shortages—or like invaders and occupiers who arrive uninvited—these systems crossed into our institutions because they promised efficiency and growth; but with little oversight, they have settled into positions of authority. But like the aristocrats who once claimed divine right or the captains of industry who insisted that the market is nature, today’s algorithmic elites insist that their power is neutral, inevitable and even benevolent. We must challenge that narrative. 

First, transparency must become the norm. If an algorithm makes a decision that affects your life—employment, credit, healthcare, parole, social media visibility—you should have the right to know how it was made. Trade secret claims should not override civil rights or due process. Oversight bodies must be given access to the models they are supposed to evaluate, as Meta’s Oversight Board has demanded. Audits should examine not only accuracy but also impacts on different communities. Public agencies using algorithms must disclose them, and private companies must be required to demonstrate fairness. 

Second, we should challenge the assumption that optimization for engagement or cost is desirable. Recommendation systems that maximize clicks often amplify harmful content. Insurance algorithms optimized for profit deny life‑saving care. Parole algorithms optimized for recidivism risk ignore rehabilitation. We should ask: optimized for whom? Models must be designed with human‑centered objectives that balance efficiency with justice and welfare. 

Third, the workers and consumers subjected to algorithmic management must have power. This means collective bargaining rights for gig workers, legal protections against algorithmic discrimination, and rights to appeal automated decisions. It means that digital serfs become digital citizens. 

Finally, we must recognize our complicity. Every time we rely on an app to deliver food, request a ride, or recommend our next video, we participate in the economy that empowers the algorithmic class. We trade data for convenience. We click “I agree” without reading. The invisible nobility thrives on our compliance. As citizens, voters and consumers, we can demand better. We can support regulations like the EU AI Act that set limits on surveillance and discrimination. We can choose platforms that prioritize transparency and ethical design. We can push companies like OpenAI to adopt governance structures that give real power to public interest directors. 

The algorithmic class is here, and it is growing. Its members don’t wear crowns, but they rule just the same. Our task is not to smash the machines, but to democratize them—to insist that algorithms serve the public good rather than narrow interests. That means setting the terms of their citizenship—transparency, accountability, and respect for human rights. In the age of invisible power, refusing to look away is the first act of resistance.

