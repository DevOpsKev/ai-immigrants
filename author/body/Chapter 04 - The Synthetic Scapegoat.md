# Chapter 4: The Synthetic Scapegoat 

Every era invents its scapegoats. 

In ancient times, a community would take a goat, symbolically load it with the sins of the people, and send it into the wilderness. The ritual—described in the Book of Leviticus—was both literal and psychological. Guilt was offloaded. Order restored. 

But it wasn’t always a goat. Sometimes it was a prisoner. Often, it was someone on the margins—a stranger, an outsider, a migrant. Someone to blame. Someone to expel. 

These weren’t metaphors. They were survival strategies—deeply human, deeply political. And they’ve never really gone away. 

Today, the scapegoat is synthetic. 

When mass layoffs hit white‑collar workers in early 2025, press releases from Workday, Autodesk and CrowdStrike sounded eerily similar: profits would *soar* because of “the increasing demand for AI”. A pivot toward artificial intelligence conveniently justified eliminating thousands of jobs while investors cheered. To casual readers it seemed the machines themselves were behind the bloodletting. But as the Associated Press pointed out, tech job postings were already down 36 percent from 2020 levels, and economists noted that the cooling job market for developers looked similar to other sectors. CEOs were folding macroeconomic realities, pandemic hangovers and shareholder expectations into a single narrative: *we had no choice—the algorithm made us do it.* 

This chapter argues that AI has become a perfect scapegoat. It cannot vote, strike, or file a lawsuit; it has no voice to contradict the narrative pinned on it. While there are real risks in delegating human decisions to software, blaming “the model” distracts us from the corporate, political and social choices driving those decisions. By naming AI scapegoating we also illuminate the emergence of a data‑rich class operating beyond democratic accountability. The real crisis is human—a crisis of accountability, agency and democratic will. 

Historically, scapegoating has been used to excuse bad policy and divide the public. Immigrant communities, in particular, have long served as convenient targets. In the United States, politicians have demonized newcomers as criminals and rapists and accused them of “taking jobs” from native‑born workers—rhetoric that Professor Mark Edberg calls an age‑old ploy to dehumanize. Canada’s federal government recently faced backlash for blaming migrants for a housing and affordability crisis even though rents rose far faster than population growth. This kind of scapegoating is not just false; it distracts from corporate profiteering and systemic failures. The current panic about “the algorithm” follows a similar script. Rather than address the political and economic forces driving inequality, leaders locate the villain in a voiceless entity—whether a person without citizenship or a machine without agency—and invite the public to vent their anger there. 

## The Perfect Fall Guy 

Why does artificial intelligence make such an attractive fall guy? For one, AI evokes both awe and unease. To those outside tech, “algorithms” appear mysterious and autonomous—machines spinning outcomes free from human influence. This narrative is useful to corporate leaders making unpopular decisions. In January 2025 Workday’s CEO claimed that “increasing demand for AI has the potential to drive a new era of growth,” while announcing layoffs that affected about 8.5 percent of staff. Autodesk’s chief executive said resources would be redeployed to AI initiatives, implying that job cuts were necessary to fund innovation. These pronouncements were widely reported without critical context; to the general public it seemed AI itself was eliminating jobs. 

The scapegoating extends well beyond tech layoffs. In housing, a handful of firms sell rent‑setting software that recommends prices across multiple landlords. These tools are now the subject of antitrust suits alleging that the software enabled coordinated price hikes. Rather than admit to collusion, landlords blame the algorithm. Lawmakers have responded with proposals to bar algorithmic price‑fixing and several cities have banned revenue‑management software. 

Insurance companies have adopted similar tactics. Major insurers face lawsuits accusing them of using algorithms to deny care. Investigations show that automatic denials are often reversed on appeal, yet very few patients know to challenge them. By attributing denials to “the system,” insurers deflect scrutiny from profit‑driven policies and downplay human oversight. 

AI scapegoating works because algorithms are, by design, inscrutable. When a person or institution cites “the algorithm” as the culprit, critics can neither cross‑examine the code nor subpoena the variables. This asymmetry empowers those who control AI systems to deflect blame onto the technology. The perfect fall guy can’t protest or sue. 

The instinct to blame a faceless other has a long pedigree. Governments and media have often pinned economic or social crises on immigrants, portraying them as job thieves or drains on public services. Canada’s decision in 2024 to slash immigration targets while rents continued to skyrocket was criticized as a “gut‑wrenching betrayal” and a dangerous deflection of blame. In the United States, labelling immigrants as criminals and asserting that they steal jobs is, as Edberg notes, an age‑old tactic. These narratives thrive because the people being blamed are marginalized and cannot easily contest the story. In the same way, AI’s inscrutability allows those in power to offload responsibility onto a tool that cannot speak for itself. Both forms of scapegoating displace attention from the policies and business models that cause harm and allow decision makers to avoid accountability. 

## Bias as a Mirror, Not a Virus 

A common narrative portrays algorithmic bias as an infection: engineers deploy machine learning systems, and somehow bias “emerges” like a virus. This metaphor casts algorithms as contaminated objects rather than mirrors reflecting human decisions. Research from University College London (UCL) and Boston University makes clear that AI does not generate prejudice from nothing; it learns our biases and often amplifies them back at us. 

In a 2024 study published in *Nature Human Behaviour*, researchers found that interacting with a biased AI system increased participants’ own bias. For example, a system trained to underestimate women’s performance caused users to undervalue women in subsequent tasks. A second experiment used the generative model Stable Diffusion to create images of “financial managers”; participants exposed to these images were significantly more likely to select a white man for the role. Co‑lead author Moshe Glickman noted that biased AIs can alter people’s beliefs and that accuracy can improve judgments only when AIs are trained and designed carefully. 

This finding is echoed by research led by Carey Morewedge at Boston University. His team found that participants detected more bias in decisions attributed to algorithms than in identical decisions attributed to themselves. Morewedge notes that algorithms learn and reproduce human biases but can also act as mirrors: when people see discrimination in a model’s output, they become more willing to correct it. In experiments with Airbnb listings, Lyft driver profiles and résumés, participants were likelier to identify and adjust bias when they thought it came from a machine rather than their own judgment. “Algorithms are a double‑edged sword,” he writes: they amplify our worst tendencies but also help us see and correct them. 

Seen this way, algorithmic bias is not some alien pathogen infecting neutral code; it is a codification of our values, data and practices. When biased algorithms produce harmful outcomes, they are reflecting systemic issues in hiring, housing, lending and policing. Blaming AI for prejudice erases the responsibility of those who built the datasets, selected the variables and decided to deploy the model. Like a funhouse mirror, AI exaggerates features we would rather ignore. Instead of smashing the mirror, we need to confront the image staring back at us. 

The same lesson applies to the stereotypes that fuel anti‑immigrant politics. Accusations that migrants are criminals, rapists or job thieves are not facts emergent from data; they are projections of longstanding fears and prejudices. As Edberg notes, such labels are “repugnant, racist, ignorant and false”, yet they persist because they serve political agendas. When biased policing or hiring data are fed into machine‑learning systems, the resulting discrimination is not a surprise; it is a mathematical echo of the bigotry that already exists. Recognizing bias as a mirror—not a virus—forces us to confront both our algorithmic tools and the social narratives that shape them. 

## Blame the Model, Not the Mogul 

The language of “the algorithm” provides a convenient scapegoat for corporate actors who want to avoid owning the consequences of their decisions. When Amazon’s hiring algorithm downgraded résumés with female markers, the company said it scrapped the system. But the problem was never the code alone; Amazon’s historical hiring data—dominated by men—taught the algorithm to favor male candidates. Amazon managers could have acknowledged the underlying sexism in their workforce and addressed it, yet the narrative focused on a rogue algorithm gone bad. 

This pattern appears in other sectors. Health insurance companies rely on algorithms to predict patient care needs and determine coverage, but when these models deny legitimate claims, insurers act as though the machine is the culprit. In reality, executives decide how much denial risk is acceptable and whether appeals will be burdensome. Similarly, RealPage’s rent‑setting software collects data from multiple landlords to recommend prices. Landlords and software makers insist they are not colluding because “the algorithm” sets prices; antitrust officials disagree. By attributing price hikes to AI, landlords mask the underlying goal: maximizing profits. 

Automated content moderation and recommendation systems also enable scapegoating. Social media companies claim that algorithmic feeds—not business models—are responsible for radicalization, misinformation and polarization. In truth, algorithms are optimized to maximize user engagement because that drives advertising revenue. A 2025 review on the neurophysiological impact of social media algorithms explains that AI‑driven recommendation engines are designed solely to capture attention for profit, altering dopamine pathways in users’ brains and fostering dependency analogous to substance addiction. 

These systems continually tailor feeds to individual preferences, maximizing screen time and deepening activation of the brain’s reward centers. When mental health crises result, platform executives point to the abstract “algorithm” rather than acknowledging a business model built on addictive design. Teen social media use illustrates this tension: 96 percent of U.S. teens go online daily and nearly half report being online “almost constantly”; yet, when negative content leaves users depressed, the companies fault the feed rather than their engagement‑maximizing incentives. 

Blame the model, not the mogul—so the mantra goes. But models do not choose to fire people, raise rents, deny medical claims or addict teenagers. Those decisions are made by people and corporations who design, deploy and profit from the systems. Shifting blame to AI deflects scrutiny away from corporate accountability and the policies that allow these practices. 

## The Rise of the Post‑Democratic Class 

The ability to offload responsibility onto AI is not simply a rhetorical trick; it is part of a broader shift toward a data‑rich elite that operates beyond democratic checks. AI systems require massive datasets, compute infrastructure and engineering expertise. Only a handful of companies—Google, Amazon, Microsoft, Meta, Tesla and their partners—control these resources. In the *Artificial Power 2025* report, the AI Now Institute warns that if we accept the trajectory offered by Big Tech as “inevitable,” we face a future that disenfranchises large sections of the public, renders systems more obscure, devalues crafts and undermines security. The report argues that AI is fundamentally about concentrating power in the hands of a few firms. These companies “shore up existing advantages” by amassing data, compute and talent; the bigger‑is‑better paradigm aligns with their incentives and squeezes out smaller competitors. 

This consolidation creates what scholars describe as a **post‑democratic class**—an elite whose control of data and compute allows them to shape society while evading oversight. The AI Now Institute warns that tech oligarchs are “counting on a wholesale rewriting of our social and economic foundations”. Economist Simon Johnson notes that the vast majority of investment in AI is concentrated in a few nations, giving the global tech industry unparalleled power. 

AI‑powered surveillance often targets workers and marginalized groups, while executives remain largely unmonitored. Gig workers delivering food or driving ride‑share vehicles are subject to algorithmic management that determines pay and termination without due process, yet the CEOs of data‑rich platforms sit outside the reach of democratic oversight. When platforms leak personal data or chatbots hallucinate, executives highlight the complexity of the technology rather than their own negligence, using AI as a shield to protect their prerogatives. 

The combination of concentrated power and scapegoating fosters a post‑democratic environment: decisions affecting millions are made within boardrooms and data centers, insulated from public scrutiny. This class wields algorithms as instruments of governance, substituting computational authority for democratic deliberation. Just as corporations hide behind AI, politicians hide behind vulnerable human targets. When Canada’s government responded to soaring rents by cutting immigration and suggesting migrants were to blame, advocates pointed out that population growth of just 3.9 percent could not explain a 20 percent rent surge. In the U.S., politicians have long portrayed immigrants as criminals and job stealers despite evidence to the contrary. Casting migrants as culprits allows leaders to avoid confronting corporate profiteering and housing shortages. Likewise, executives who blame “the algorithm” dodge accountability for choices driven by profit, thereby reinforcing a culture of scapegoating that obscures the real centers of power. 

## Social Media and Learned Helplessness 

Social media platforms illustrate how AI‑driven systems foster learned helplessness while distracting users with algorithmic scapegoats. A difference‑in‑differences study using Dutch data found that Instagram’s introduction of an algorithmic feed led to poorer mental health among teenagers because the feed rewarded negative social comparisons. A 2025 neurophysiological review further describes how recommendation engines are designed to maximize attention; by modulating dopamine pathways they encourage compulsive engagement and heighten emotional sensitivity. MIT researchers observed a feedback loop in which users with mental health symptoms consumed more negative content, which then worsened their symptoms. 

Instead of addressing the profit motives behind these designs, society often blames the “feed” itself. Politicians hold hearings about harmful algorithms; parents lament that TikTok’s recommendations are destroying their children’s attention spans. But the underlying business model—selling targeted ads by capturing attention—is rarely questioned. In 2024 nearly half of U.S. teenagers reported being online almost constantly and 96 percent went online every day. When youths become depressed, we pin the blame on an algorithm rather than on companies that intentionally engineer addictive environments. 

AI‑driven platforms also amplify cultural self‑loathing. Instagram and TikTok serve up endless highlight reels of others’ lives, prompting users to measure themselves against unrealistic beauty and success standards. The constant barrage of curated content fosters feelings of inadequacy and body image issues; a meta‑analysis found a 13 percent increase in depression risk for each additional hour spent on social media. As more AI tools generate hyperreal faces and bodies, the gap between digital perfection and lived reality grows. This, too, is blamed on “the algorithm,” not the industries monetizing insecurity. 

The feeds also amplify political scapegoating. Recommendation engines optimize for outrage and engagement, so inflammatory posts about immigration or crime rise to the top. False claims that migrants are criminals or that immigration causes unemployment circulate widely, even though research shows immigrants are less likely to commit crimes and often complement native labor. When users become fearful or resentful after consuming these narratives, they may blame “the algorithm” for radicalizing them. Yet those narratives reflect a deliberate choice by platform operators to boost polarizing content because division drives clicks. The same infrastructure that fuels self‑loathing can also fuel xenophobia, deepening social divisions while shielding the corporations profiting from them. 

Learned helplessness arises when individuals feel they have no control over their environment. Users know that social media feeds are engineered to be addictive and that negative content harms them, yet they continue scrolling because the platforms offer few alternatives and social life has migrated online. By portraying AI systems as autonomous forces, platform owners encourage this helplessness. If the feed is in control, why bother resisting? The scapegoat narrative invites resignation. 

## We Sleepwalked Into This 

It is tempting to frame AI scapegoating solely as a top‑down phenomenon: corporate moguls shift blame onto machines, and the public naively accepts it. Yet we all share some responsibility. Our hunger for convenience, entertainment and personalization created the market for algorithmic systems. Millions of users freely train recommendation engines with their data; managers adopt AI hiring tools to process tens of thousands of applications because human review would be slower; doctors acquiesce to algorithmic risk scores to manage workloads. When we accept biased recommendations, personalized pricing and automated denials as the cost of participation, we normalise the delegation of judgment to machines. 

Survey data show that by 2024 smartphone ownership among U.S. teens reached 95 percent. Many teens described their usage as “almost constant,” with 73 percent visiting YouTube daily and 12 percent reporting near‑constant use of Instagram. Social life now unfolds in algorithmic spaces. It is no wonder that individuals struggle to imagine alternatives; the digital infrastructure for community, work and politics is privately owned and optimized for profit. We are not just passive victims of AI scapegoating; we have, through inattention and convenience, contributed to the very systems that now govern us. 

Our passivity has also allowed anti‑immigrant scapegoating to flourish. Many voters accept claims that migrants cause crime or housing shortages without examining the evidence or the motives of those making such assertions. By failing to challenge xenophobic narratives or hold leaders accountable, we contribute to a political environment in which blaming the vulnerable is normalized. That same reflex now emerges in discussions about AI: instead of interrogating corporate agendas or public policy, we blame a technology we barely understand. Naming these parallels is not about equating the plight of migrants with the status of software; it is about recognizing a consistent pattern of deflection and the ways our complacency enables it. 

Similarly, voters often treat complex social problems as technical puzzles to be solved by AI. Policymakers tout automated monitoring for everything from pandemic contact tracing to welfare fraud detection. In doing so, we outsource moral and political decisions to algorithms. When those systems inevitably replicate existing inequities, we act surprised, as if the code were to blame. But if we refuse to name the political and economic forces shaping these systems—deregulation, austerity, shareholder primacy—we cannot hope to change them. 

## Reclaiming Responsibility 

What would it look like to stop scapegoating AI and instead reclaim responsibility? We must insist on transparency and accountability from those who deploy algorithms. If a company uses a model to set prices or approve care, regulators should know what data it uses and how it weighs outcomes. Litigation and legislation targeting algorithmic collusion and automated denials are steps in this direction, but transparency will matter only if coupled with reforms that address underlying incentives. We also need to treat bias as a reflection of systemic inequities and reform the data and practices that create it. Finally, we must challenge the concentration of data and compute that underpins the post‑democratic class. The AI Now Institute warns against ceding the future to a handful of oligarchs, and economists caution that without guardrails, inequality will deepen. Policies such as public investment in open infrastructure, antitrust enforcement and cooperative ownership can counterbalance these forces. 

Finally, we need to cultivate agency at the individual and community levels. Mental health research shows that labeling web pages according to their emotional impact helps users choose healthier content. Similar tools could empower users to take control of their digital diets and resist doomscrolling. Education about algorithmic design, bias and incentives can demystify AI and reduce fatalism. Collective action—through unions, consumer cooperatives and political organizing—can pressure companies to prioritize human well‑being over short‑term profits. Only by reclaiming agency can we prevent learned helplessness and cultural self‑loathing from becoming the default. 

Reclaiming responsibility also means challenging scapegoat politics wherever it appears. When politicians deflect blame onto migrants for housing shortages or job losses despite evidence showing immigrants strengthen economies, we must call out the falsehood and demand policies that address the real causes—corporate greed, austerity and deregulation. And when executives blame AI for layoffs, discrimination or addiction, we must direct scrutiny back to their business models and incentives. Solidarity between human communities and those who build and use AI is essential: no one should be left to absorb blame for systemic failures. By resisting scapegoating in all its forms, we reaffirm our commitment to democratic accountability and human dignity. 

## Conclusion: The Proxy Problem 

AI is not the problem—it is the proxy. Scapegoating artificial intelligence allows corporations, governments and societies to avoid confronting the human choices that shape our world. By blaming the algorithm, leaders hide the fact that they chose profit over people in layoffs, pricing, and care. By treating bias as an algorithmic bug, institutions ignore the historical prejudices embedded in their data and practices. By focusing on the technology, we fail to see the rise of a post‑democratic class whose power derives from concentrated data and weak accountability. And by blaming the feed, we overlook the business models that engineer addiction and division. 

A scapegoat is useful because it can absorb collective guilt without fighting back. But scapegoating AI leaves us powerless to change the systems that harm us. If we refuse to name the corporate actors, public policies and social habits driving these outcomes, we cannot hope to fix them. Accountability requires pulling back the curtain and seeing AI not as an autonomous agent but as a tool wielded by humans with particular incentives. The mission of our time is to reclaim responsibility—for the biases we codify, the business models we accept, the data we surrender and the political structures we legitimize. Only then can we transform AI from a synthetic scapegoat into a shared resource that serves the public good. 
